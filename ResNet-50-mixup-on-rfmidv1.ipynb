{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":2530487,"sourceType":"datasetVersion","datasetId":1533360},{"sourceId":13811294,"sourceType":"datasetVersion","datasetId":8794300},{"sourceId":13894314,"sourceType":"datasetVersion","datasetId":8852034}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nfrom typing import Dict\n\ndef harmonize_odir(split: str) -> pd.DataFrame:\n    base_path = \"/kaggle/input/odir-clr/ODIR_CLR\"\n    \n    if split == \"train\":\n        img_dir = f\"{base_path}/Training_ Set/train_images\"\n        label_file = f\"{base_path}/Training_ Set/train_annotation.xlsx\"\n    elif split == \"val\":\n        img_dir = f\"{base_path}/Validation_set/val_images\"\n        label_file = f\"{base_path}/Validation_set/val_annotation.xlsx\"\n    else:\n        img_dir = f\"{base_path}/Test_Set/test_images\"\n        label_file = f\"{base_path}/Test_Set/test_annotation.xlsx\"\n    \n    df = pd.read_excel(label_file)\n    label_cols = ['N', 'D', 'G', 'C', 'A', 'H', 'M', 'O']\n    harmonized_rows = []\n    \n    for idx, row in df.iterrows():\n        patient_id = row['ID']\n        labels = {col: int(row[col]) for col in label_cols}\n        \n        if pd.notna(row['Left-Fundus']) and str(row['Left-Fundus']).strip():\n            left_path = os.path.join(img_dir, f\"{patient_id}_left.jpg\")\n            harmonized_rows.append({\n                'image_path': left_path,\n                'dataset': 'ODIR',\n                'split': split,\n                'ID': patient_id,\n                **labels\n            })\n        \n        if pd.notna(row['Right-Fundus']) and str(row['Right-Fundus']).strip():\n            right_path = os.path.join(img_dir, f\"{patient_id}_right.jpg\")\n            harmonized_rows.append({\n                'image_path': right_path,\n                'dataset': 'ODIR',\n                'split': split,\n                'ID': patient_id,\n                **labels\n            })\n    \n    return pd.DataFrame(harmonized_rows)\n\n\ndef harmonize_rfmid_v1(split: str) -> pd.DataFrame:\n    base_path = \"/kaggle/input/retinal-disease-classification\"\n    \n    if split == \"train\":\n        img_dir = f\"{base_path}/Training_Set/Training_Set/Training\"\n        label_file = f\"{base_path}/Training_Set/Training_Set/RFMiD_Training_Labels.csv\"\n    elif split == \"val\":\n        img_dir = f\"{base_path}/Evaluation_Set/Evaluation_Set/Validation\"\n        label_file = f\"{base_path}/Evaluation_Set/Evaluation_Set/RFMiD_Validation_Labels.csv\"\n    else:\n        img_dir = f\"{base_path}/Test_Set/Test_Set/Test\"\n        label_file = f\"{base_path}/Test_Set/Test_Set/RFMiD_Testing_Labels.csv\"\n    \n    df = pd.read_csv(label_file)\n    \n    all_disease_cols = ['DR', 'ARMD', 'MH', 'DN', 'MYA', 'BRVO', 'TSLN',\n                        'ERM', 'LS', 'MS', 'CSR', 'ODC', 'CRVO', 'TV', 'AH',\n                        'ODP', 'ODE', 'ST', 'AION', 'PT', 'RT', 'RS', 'CRS',\n                        'EDN', 'RPEC', 'MHL', 'RP', 'CWS', 'CB', 'ODPM',\n                        'PRH', 'MNF', 'HR', 'CRAO', 'TD', 'CME', 'PTCR', 'CF',\n                        'VH', 'MCA', 'VS', 'BRAO', 'PLQ', 'HPED', 'CL']\n    \n    harmonized_rows = []\n    \n    for idx, row in df.iterrows():\n        image_id = row['ID']\n        image_path = None\n        for ext in ['.png', '.jpg', '.jpeg']:\n            candidate = os.path.join(img_dir, f\"{image_id}{ext}\")\n            if os.path.exists(candidate):\n                image_path = candidate\n                break\n        if image_path is None:\n            image_path = os.path.join(img_dir, str(image_id))\n        \n        # Direct single-column mapping\n        N = 1 if row['Disease_Risk'] == 0 else 0\n        D = 1 if row['DR'] == 1 else 0\n        G = 1 if row['ODC'] == 1 else 0\n        C = 1 if row['MH'] == 1 else 0\n        A = 1 if row['ARMD'] == 1 else 0\n        H = 1 if row['HR'] == 1 else 0\n        M = 1 if row['MYA'] == 1 else 0\n        \n        # Other: any disease column is 1 except those used for N,D,G,C,A,H,M\n        used_for_mapping = ['DR', 'ODC', 'MH', 'ARMD', 'HR', 'MYA']\n        other_cols = [col for col in all_disease_cols if col not in used_for_mapping]\n        O = 1 if any(row[col] == 1 for col in other_cols) else 0\n        \n        harmonized_rows.append({\n            'image_path': image_path,\n            'dataset': 'RFMiD_v1',\n            'split': split,\n            'ID': image_id,\n            'N': N, 'D': D, 'G': G, 'C': C, 'A': A, 'H': H, 'M': M, 'O': O\n        })\n    \n    return pd.DataFrame(harmonized_rows)\n\n\ndef harmonize_rfmid_v2(split: str) -> pd.DataFrame:\n    base_path = \"/kaggle/input/rdc-version-2/RFDiM2_0\"\n    \n    if split == \"train\":\n        img_dir = f\"{base_path}/Training_set_2/Train_2\"\n        label_file = f\"{base_path}/Training_set_2/RFMiD_2_Training_labels.csv\"\n    elif split == \"val\":\n        img_dir = f\"{base_path}/Validation_set_2/Validation_2\"\n        label_file = f\"{base_path}/Validation_set_2/RFMiD_2_Validation_labels.csv\"\n    else:\n        img_dir = f\"{base_path}/Test_set_2/Test_2\"\n        label_file = f\"{base_path}/Test_set_2/RFMiD_2_Testing_labels.csv\"\n    \n    try:\n        df = pd.read_csv(label_file, encoding='utf-8')\n    except UnicodeDecodeError:\n        df = pd.read_csv(label_file, encoding='latin1')\n    \n    df.columns = df.columns.str.strip()\n    \n    potential_disease_cols = ['AH', 'AION', 'ARMD', 'BRVO', 'CB', 'CF', 'CL', 'CME',\n                              'CNV', 'CRAO', 'CRS', 'CRVO', 'CSR', 'CWS', 'CSC', 'DN', \n                              'DR', 'EDN', 'ERM', 'GRT', 'HPED', 'HR', 'LS', 'MCA', \n                              'ME', 'MH', 'MHL', 'MS', 'MYA', 'ODC', 'ODE', 'ODP', \n                              'ON', 'OPDM', 'PRH', 'RD', 'RHL', 'RTR', 'RP', 'RPEC', \n                              'RS', 'RT', 'SOFE', 'ST', 'TD', 'TSLN', 'TV', 'VS', \n                              'HTN', 'IIH']\n    \n    all_disease_cols = [col for col in potential_disease_cols if col in df.columns]\n    harmonized_rows = []\n    \n    skipped_count = 0\n    found_count = 0\n    \n    for idx, row in df.iterrows():\n        # Convert to int directly here!\n        image_id = int(row['ID'])\n        \n        # Try to find the image file with exact filename match\n        image_path = None\n        \n        for ext in ['.jpg', '.JPG', '.png', '.PNG', '.jpeg', '.JPEG']:\n            candidate = os.path.join(img_dir, f\"{image_id}{ext}\")\n            if os.path.exists(candidate):\n                image_path = candidate\n                found_count += 1\n                break\n        \n        # Skip if not found\n        if image_path is None:\n            skipped_count += 1\n            continue\n        \n        # Map labels (v2 uses WNL for Normal)\n        wnl = row.get('WNL', 0)\n        N = 1 if wnl == 1 else 0\n        D = 1 if row.get('DR', 0) == 1 else 0\n        G = 1 if row.get('ODC', 0) == 1 else 0\n        C = 1 if row.get('MH', 0) == 1 else 0\n        A = 1 if row.get('ARMD', 0) == 1 else 0\n        H = 1 if row.get('HTN', 0) == 1 else 0\n        if H == 0 and 'HR' in df.columns:\n            H = 1 if row.get('HR', 0) == 1 else 0\n        M = 1 if row.get('MYA', 0) == 1 else 0\n        \n        # Other: any disease except mapped ones\n        used_for_mapping = ['DR', 'ODC', 'MH', 'ARMD', 'HTN', 'HR', 'MYA', 'WNL']\n        other_cols = [col for col in all_disease_cols if col not in used_for_mapping]\n        O = 1 if any(row.get(col, 0) == 1 for col in other_cols) else 0\n        \n        harmonized_rows.append({\n            'image_path': image_path,\n            'dataset': 'RFMiD_v2',\n            'split': split,\n            'ID': image_id,\n            'N': N, 'D': D, 'G': G, 'C': C, 'A': A, 'H': H, 'M': M, 'O': O\n        })\n    \n    if skipped_count > 0:\n        print(f\"RFMiD_v2 {split}: Found {found_count}, Skipped {skipped_count}\")\n    \n    return pd.DataFrame(harmonized_rows)\n\n\ndef harmonize_all_datasets() -> Dict[str, pd.DataFrame]:\n    results = {}\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"HARMONIZING DATASETS\")\n    print(\"=\"*60)\n    \n    for split in ['train', 'val', 'test']:\n        print(f\"\\nProcessing {split} split...\")\n        results[f'ODIR_{split}'] = harmonize_odir(split)\n        results[f'RFMiD_v1_{split}'] = harmonize_rfmid_v1(split)\n        results[f'RFMiD_v2_{split}'] = harmonize_rfmid_v2(split)\n    \n    return results\n\n\ndef print_statistics(harmonized_data: Dict[str, pd.DataFrame]):\n    label_cols = ['N', 'D', 'G', 'C', 'A', 'H', 'M', 'O']\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"DATASET STATISTICS\")\n    print(\"=\"*60)\n    \n    print(\"\\nðŸ“Š Sample Counts:\")\n    print(\"-\" * 40)\n    for key, df in sorted(harmonized_data.items()):\n        print(f\"{key:20s}: {len(df):5d} images\")\n    \n    # Calculate totals\n    total_train = sum(len(df) for k, df in harmonized_data.items() if 'train' in k)\n    total_val = sum(len(df) for k, df in harmonized_data.items() if 'val' in k)\n    total_test = sum(len(df) for k, df in harmonized_data.items() if 'test' in k)\n    \n    print(\"-\" * 40)\n    print(f\"{'Total Train':20s}: {total_train:5d} images\")\n    print(f\"{'Total Val':20s}: {total_val:5d} images\")\n    print(f\"{'Total Test':20s}: {total_test:5d} images\")\n    print(f\"{'Grand Total':20s}: {total_train + total_val + total_test:5d} images\")\n    \n    for dataset_name in ['ODIR', 'RFMiD_v1', 'RFMiD_v2']:\n        dataset_dfs = {k: v for k, v in harmonized_data.items() if k.startswith(dataset_name)}\n        if not dataset_dfs:\n            continue\n        \n        print(f\"\\n{'='*60}\")\n        print(f\"{dataset_name} Label Distribution\")\n        print(f\"{'='*60}\")\n        \n        for split in ['train', 'val', 'test']:\n            key = f\"{dataset_name}_{split}\"\n            if key not in dataset_dfs:\n                continue\n            \n            df = dataset_dfs[key]\n            print(f\"\\n{split.upper()} ({len(df)} images):\")\n            print(\"-\" * 40)\n            for col in label_cols:\n                count = df[col].sum()\n                pct = (count / len(df)) * 100 if len(df) > 0 else 0\n                print(f\"  {col}: {count:5d} ({pct:5.1f}%)\")\n\n\ndef save_harmonized_data(harmonized_data: Dict[str, pd.DataFrame], output_dir: str = './harmonized_labels'):\n    os.makedirs(output_dir, exist_ok=True)\n    \n    print(\"\\n\" + \"=\"*60)\n    print(\"SAVING HARMONIZED DATA\")\n    print(\"=\"*60)\n    \n    # Save individual dataset files\n    for key, df in harmonized_data.items():\n        output_path = os.path.join(output_dir, f\"{key}.csv\")\n        df.to_csv(output_path, index=False)\n        print(f\"âœ… Saved {key:20s}: {len(df):5d} rows â†’ {output_path}\")\n    \n    # Save combined files\n    print(\"\\nðŸ“¦ Creating combined files...\")\n    for split in ['train', 'val', 'test']:\n        split_dfs = [v for k, v in harmonized_data.items() if k.endswith(f'_{split}')]\n        if split_dfs:\n            combined = pd.concat(split_dfs, ignore_index=True)\n            output_path = os.path.join(output_dir, f\"combined_{split}.csv\")\n            combined.to_csv(output_path, index=False)\n            print(f\"âœ… Saved combined_{split:5s}: {len(combined):5d} rows â†’ {output_path}\")\n    \n    print(f\"\\nâœ¨ All files saved to: {output_dir}\")\n\n\ndef verify_images(harmonized_data: Dict[str, pd.DataFrame]):\n    \"\"\"Verify that all image paths in the harmonized data actually exist\"\"\"\n    print(\"\\n\" + \"=\"*60)\n    print(\"VERIFYING IMAGE PATHS\")\n    print(\"=\"*60)\n    \n    all_good = True\n    for key, df in sorted(harmonized_data.items()):\n        missing = df[~df['image_path'].apply(os.path.exists)]\n        if len(missing) > 0:\n            print(f\"âŒ {key:20s}: {len(missing)} missing images\")\n            all_good = False\n        else:\n            print(f\"âœ… {key:20s}: All {len(df)} images found\")\n    \n    if all_good:\n        print(\"\\nðŸŽ‰ All image paths verified successfully!\")\n    else:\n        print(\"\\nâš ï¸  Some images are missing - check the paths above\")\n\n\nif __name__ == \"__main__\":\n    # Harmonize all datasets\n    harmonized_data = harmonize_all_datasets()\n    \n    # Print statistics\n    print_statistics(harmonized_data)\n    \n    # Verify all images exist\n    verify_images(harmonized_data)\n    \n    # Save to CSV files\n    save_harmonized_data(harmonized_data)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-17T17:19:10.803307Z","iopub.execute_input":"2026-01-17T17:19:10.803545Z","iopub.status.idle":"2026-01-17T17:20:00.339344Z","shell.execute_reply.started":"2026-01-17T17:19:10.803521Z","shell.execute_reply":"2026-01-17T17:20:00.338785Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\nHARMONIZING DATASETS\n============================================================\n\nProcessing train split...\nRFMiD_v2 train: Found 507, Skipped 2\n\nProcessing val split...\n\nProcessing test split...\nRFMiD_v2 test: Found 170, Skipped 4\n\n============================================================\nDATASET STATISTICS\n============================================================\n\nðŸ“Š Sample Counts:\n----------------------------------------\nODIR_test           :  2000 images\nODIR_train          :  7000 images\nODIR_val            :  1000 images\nRFMiD_v1_test       :   640 images\nRFMiD_v1_train      :  1920 images\nRFMiD_v1_val        :   640 images\nRFMiD_v2_test       :   170 images\nRFMiD_v2_train      :   507 images\nRFMiD_v2_val        :   177 images\n----------------------------------------\nTotal Train         :  9427 images\nTotal Val           :  1817 images\nTotal Test          :  2810 images\nGrand Total         : 14054 images\n\n============================================================\nODIR Label Distribution\n============================================================\n\nTRAIN (7000 images):\n----------------------------------------\n  N:  2280 ( 32.6%)\n  D:  2256 ( 32.2%)\n  G:   430 (  6.1%)\n  C:   424 (  6.1%)\n  A:   328 (  4.7%)\n  H:   206 (  2.9%)\n  M:   348 (  5.0%)\n  O:  1958 ( 28.0%)\n\nVAL (1000 images):\n----------------------------------------\n  N:   324 ( 32.4%)\n  D:   326 ( 32.6%)\n  G:    64 (  6.4%)\n  C:    62 (  6.2%)\n  A:    50 (  5.0%)\n  H:    32 (  3.2%)\n  M:    46 (  4.6%)\n  O:   272 ( 27.2%)\n\nTEST (2000 images):\n----------------------------------------\n  N:   648 ( 32.4%)\n  D:   654 ( 32.7%)\n  G:   116 (  5.8%)\n  C:   130 (  6.5%)\n  A:    98 (  4.9%)\n  H:    60 (  3.0%)\n  M:    92 (  4.6%)\n  O:   550 ( 27.5%)\n\n============================================================\nRFMiD_v1 Label Distribution\n============================================================\n\nTRAIN (1920 images):\n----------------------------------------\n  N:   401 ( 20.9%)\n  D:   376 ( 19.6%)\n  G:   282 ( 14.7%)\n  C:   317 ( 16.5%)\n  A:   100 (  5.2%)\n  H:     0 (  0.0%)\n  M:   101 (  5.3%)\n  O:   785 ( 40.9%)\n\nVAL (640 images):\n----------------------------------------\n  N:   134 ( 20.9%)\n  D:   132 ( 20.6%)\n  G:    72 ( 11.2%)\n  C:   102 ( 15.9%)\n  A:    38 (  5.9%)\n  H:     0 (  0.0%)\n  M:    34 (  5.3%)\n  O:   271 ( 42.3%)\n\nTEST (640 images):\n----------------------------------------\n  N:   134 ( 20.9%)\n  D:   124 ( 19.4%)\n  G:    91 ( 14.2%)\n  C:   104 ( 16.2%)\n  A:    31 (  4.8%)\n  H:     1 (  0.2%)\n  M:    32 (  5.0%)\n  O:   257 ( 40.2%)\n\n============================================================\nRFMiD_v2 Label Distribution\n============================================================\n\nTRAIN (507 images):\n----------------------------------------\n  N:   156 ( 30.8%)\n  D:    42 (  8.3%)\n  G:    21 (  4.1%)\n  C:    26 (  5.1%)\n  A:     5 (  1.0%)\n  H:    64 ( 12.6%)\n  M:    27 (  5.3%)\n  O:   256 ( 50.5%)\n\nVAL (177 images):\n----------------------------------------\n  N:    53 ( 29.9%)\n  D:    14 (  7.9%)\n  G:     9 (  5.1%)\n  C:     8 (  4.5%)\n  A:     3 (  1.7%)\n  H:    13 (  7.3%)\n  M:    11 (  6.2%)\n  O:    89 ( 50.3%)\n\nTEST (170 images):\n----------------------------------------\n  N:    52 ( 30.6%)\n  D:    14 (  8.2%)\n  G:     7 (  4.1%)\n  C:     7 (  4.1%)\n  A:     2 (  1.2%)\n  H:    18 ( 10.6%)\n  M:     5 (  2.9%)\n  O:    89 ( 52.4%)\n\n============================================================\nVERIFYING IMAGE PATHS\n============================================================\nâœ… ODIR_test           : All 2000 images found\nâœ… ODIR_train          : All 7000 images found\nâœ… ODIR_val            : All 1000 images found\nâœ… RFMiD_v1_test       : All 640 images found\nâœ… RFMiD_v1_train      : All 1920 images found\nâœ… RFMiD_v1_val        : All 640 images found\nâœ… RFMiD_v2_test       : All 170 images found\nâœ… RFMiD_v2_train      : All 507 images found\nâœ… RFMiD_v2_val        : All 177 images found\n\nðŸŽ‰ All image paths verified successfully!\n\n============================================================\nSAVING HARMONIZED DATA\n============================================================\nâœ… Saved ODIR_train          :  7000 rows â†’ ./harmonized_labels/ODIR_train.csv\nâœ… Saved RFMiD_v1_train      :  1920 rows â†’ ./harmonized_labels/RFMiD_v1_train.csv\nâœ… Saved RFMiD_v2_train      :   507 rows â†’ ./harmonized_labels/RFMiD_v2_train.csv\nâœ… Saved ODIR_val            :  1000 rows â†’ ./harmonized_labels/ODIR_val.csv\nâœ… Saved RFMiD_v1_val        :   640 rows â†’ ./harmonized_labels/RFMiD_v1_val.csv\nâœ… Saved RFMiD_v2_val        :   177 rows â†’ ./harmonized_labels/RFMiD_v2_val.csv\nâœ… Saved ODIR_test           :  2000 rows â†’ ./harmonized_labels/ODIR_test.csv\nâœ… Saved RFMiD_v1_test       :   640 rows â†’ ./harmonized_labels/RFMiD_v1_test.csv\nâœ… Saved RFMiD_v2_test       :   170 rows â†’ ./harmonized_labels/RFMiD_v2_test.csv\n\nðŸ“¦ Creating combined files...\nâœ… Saved combined_train:  9427 rows â†’ ./harmonized_labels/combined_train.csv\nâœ… Saved combined_val  :  1817 rows â†’ ./harmonized_labels/combined_val.csv\nâœ… Saved combined_test :  2810 rows â†’ ./harmonized_labels/combined_test.csv\n\nâœ¨ All files saved to: ./harmonized_labels\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, ConcatDataset\n\nimport torchvision.transforms as transforms\nimport torchvision.models as models\n\nfrom sklearn.metrics import roc_auc_score, f1_score, roc_curve, average_precision_score\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n\n# =============================================================================\n# Config for FOLD (Swapped: Train=ODIR+RFMiD_v2, Test=RFMiD_v1)\n# =============================================================================\n\nSEED = 42\nSAVE_DIR = './results_lodo_mixup/fold_test_RFMiD_v1'\nTEST_DOMAIN = \"RFMiD_v1\"\nTRAIN_DOMAINS = \"ODIR + RFMiD_v2\"\n\n# Mixup hyperparameter\nMIXUP_ALPHA = 0.2  # Controls mixing strength (0.2 is standard)\n\n# Paths (SWAPPED ROLES)\nTEST_CSV = '/kaggle/working/harmonized_labels/RFMiD_v1_test.csv'\nTRAIN_CSVS = [\n    '/kaggle/working/harmonized_labels/ODIR_train.csv',\n    '/kaggle/working/harmonized_labels/RFMiD_v2_train.csv'\n]\nVAL_CSVS = [\n    '/kaggle/working/harmonized_labels/ODIR_val.csv',\n    '/kaggle/working/harmonized_labels/RFMiD_v2_val.csv'\n]\n\n# =============================================================================\n# Reproducibility\n# =============================================================================\n\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n\n# =============================================================================\n# Dataset (with domain tracking for Mixup!)\n# =============================================================================\n\nclass RetinalDataset(Dataset):\n    def __init__(self, csv_path, domain_id, transform=None):\n        self.data = pd.read_csv(csv_path)\n        self.domain_id = domain_id  # Track which domain this is\n        self.transform = transform\n        self.label_cols = [\"N\", \"D\", \"G\", \"C\", \"A\", \"H\", \"M\", \"O\"]\n\n        dup = self.data.duplicated(subset=[\"image_path\"], keep=\"first\").sum()\n        if dup > 0:\n            print(f\"[WARN] {dup} duplicates found, keeping first\")\n        self.data = self.data.drop_duplicates(subset=[\"image_path\"]).reset_index(drop=True)\n\n        valid_mask = self.data[\"image_path\"].apply(os.path.exists)\n        missing = (~valid_mask).sum()\n        if missing > 0:\n            print(f\"[WARN] Dropping {missing} missing images\")\n        self.data = self.data.loc[valid_mask].reset_index(drop=True)\n        print(f\"[INFO] Domain {domain_id}: Loaded {len(self.data)} valid images\")\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        row = self.data.iloc[idx]\n        try:\n            image = Image.open(row[\"image_path\"]).convert(\"RGB\")\n        except:\n            image = Image.new(\"RGB\", (224, 224), color=(128, 128, 128))\n        labels = torch.tensor(row[self.label_cols].values.astype(\"float32\"))\n        if self.transform:\n            image = self.transform(image)\n        return image, labels, self.domain_id  # Return domain_id\n\n# =============================================================================\n# Pos weight\n# =============================================================================\n\ndef calculate_pos_weights(datasets, clip_min=0.5, clip_max=50.0):\n    all_labels = []\n    for ds in datasets:\n        all_labels.append(ds.data[[\"N\", \"D\", \"G\", \"C\", \"A\", \"H\", \"M\", \"O\"]].values)\n    combined = np.vstack(all_labels)\n    pos = combined.sum(axis=0)\n    neg = len(combined) - pos\n    raw = neg / (pos + 1e-5)\n    clipped = np.clip(raw, clip_min, clip_max)\n    \n    print(\"\\n[INFO] Positive class weights (from training domains):\")\n    for i, col in enumerate([\"N\", \"D\", \"G\", \"C\", \"A\", \"H\", \"M\", \"O\"]):\n        print(f\"  {col}: {clipped[i]:.2f}\")\n    \n    return torch.tensor(clipped, dtype=torch.float32)\n\n# =============================================================================\n# Model\n# =============================================================================\n\nclass MixupMultiLabel(nn.Module):\n    def __init__(self, num_classes=8, dropout=0.3):\n        super().__init__()\n        self.backbone = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n        in_features = self.backbone.fc.in_features\n        self.backbone.fc = nn.Identity()\n        self.classifier = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Linear(in_features, 512),\n            nn.ReLU(),\n            nn.BatchNorm1d(512),\n            nn.Dropout(dropout),\n            nn.Linear(512, num_classes),\n        )\n    \n    def forward(self, x):\n        return self.classifier(self.backbone(x))\n\n# =============================================================================\n# Transforms\n# =============================================================================\n\ndef get_transforms(is_train=False):\n    if is_train:\n        return transforms.Compose([\n            transforms.Resize((256, 256)),\n            transforms.RandomCrop(224),\n            transforms.RandomRotation(10),\n            transforms.ColorJitter(brightness=0.2, contrast=0.2),\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n        ])\n    else:\n        return transforms.Compose([\n            transforms.Resize((224, 224)),\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n        ])\n\n# =============================================================================\n# Metrics\n# =============================================================================\n\ndef compute_metrics(labels, probs, thresholds=None):\n    n_classes = labels.shape[1]\n    \n    aucs = []\n    for i in range(n_classes):\n        if len(np.unique(labels[:, i])) > 1:\n            aucs.append(roc_auc_score(labels[:, i], probs[:, i]))\n        else:\n            aucs.append(np.nan)\n    \n    aps = []\n    for i in range(n_classes):\n        if len(np.unique(labels[:, i])) > 1:\n            aps.append(average_precision_score(labels[:, i], probs[:, i]))\n        else:\n            aps.append(np.nan)\n    \n    if thresholds is None:\n        thresholds = np.full(n_classes, 0.5)\n    \n    preds = (probs >= thresholds).astype(int)\n    f1 = f1_score(labels, preds, average=\"macro\", zero_division=0)\n    \n    return {\n        \"mAUC\": float(np.nanmean(aucs)),\n        \"mAP\": float(np.nanmean(aps)),\n        \"per_class_auc\": aucs,\n        \"per_class_ap\": aps,\n        \"macro_f1\": float(f1),\n    }\n\ndef find_optimal_thresholds(labels, probs):\n    n_classes = labels.shape[1]\n    thresholds = []\n    search_range = np.linspace(0.05, 0.95, 91)\n    \n    for i in range(n_classes):\n        best_f1, best_t = 0.0, 0.5\n        if len(np.unique(labels[:, i])) > 1:\n            for t in search_range:\n                preds = (probs[:, i] >= t).astype(int)\n                f1 = f1_score(labels[:, i], preds, zero_division=0)\n                if f1 > best_f1:\n                    best_f1, best_t = f1, t\n        thresholds.append(best_t)\n    \n    return np.array(thresholds)\n\n# =============================================================================\n# MIXUP TRAINING (FIXED!)\n# =============================================================================\n\ndef train_epoch_mixup(model, loaders_dict, criterion, optimizer, device, mixup_alpha=0.2):\n    \"\"\"\n    Train with inter-domain Mixup (FIXED VERSION)\n    \n    Args:\n        loaders_dict: Dict of {domain_id: DataLoader}\n        mixup_alpha: Beta distribution parameter (default 0.2)\n    \"\"\"\n    model.train()\n    losses = []\n    mixup_stats = {'total_batches': 0, 'mixup_batches': 0}\n    \n    # Create iterators for each domain\n    domain_iters = {k: iter(v) for k, v in loaders_dict.items()}\n    domain_ids = list(loaders_dict.keys())\n    \n    # Calculate total batches (use the maximum)\n    max_batches = max(len(loader) for loader in loaders_dict.values())\n    \n    pbar = tqdm(range(max_batches), desc=\"Train (Mixup)\", leave=False)\n    \n    for _ in pbar:\n        # Sample pairs of domains for mixup\n        if len(domain_ids) >= 2 and np.random.rand() > 0.5:  # 50% chance of mixup\n            # Randomly pick two different domains\n            d1, d2 = np.random.choice(domain_ids, size=2, replace=False)\n            \n            # Get batches from each domain\n            try:\n                imgs1, labels1, _ = next(domain_iters[d1])\n            except StopIteration:\n                domain_iters[d1] = iter(loaders_dict[d1])\n                imgs1, labels1, _ = next(domain_iters[d1])\n            \n            try:\n                imgs2, labels2, _ = next(domain_iters[d2])\n            except StopIteration:\n                domain_iters[d2] = iter(loaders_dict[d2])\n                imgs2, labels2, _ = next(domain_iters[d2])\n            \n            # FIXED: Ensure same batch size\n            min_size = min(imgs1.size(0), imgs2.size(0))\n            imgs1, labels1 = imgs1[:min_size], labels1[:min_size]\n            imgs2, labels2 = imgs2[:min_size], labels2[:min_size]\n            \n            # Sample lambda from Beta distribution\n            lam = np.random.beta(mixup_alpha, mixup_alpha)\n            \n            # Mix images and labels\n            mixed_imgs = lam * imgs1 + (1 - lam) * imgs2\n            mixed_labels = lam * labels1 + (1 - lam) * labels2\n            \n            mixed_imgs = mixed_imgs.to(device)\n            mixed_labels = mixed_labels.to(device)\n            \n            # Forward pass\n            optimizer.zero_grad()\n            logits = model(mixed_imgs)\n            \n            # Multi-label BCE loss (works directly with mixed labels!)\n            loss = criterion(logits, mixed_labels)\n            \n            mixup_stats['mixup_batches'] += 1\n            \n        else:\n            # Standard training (no mixup)\n            d = np.random.choice(domain_ids)\n            try:\n                imgs, labels, _ = next(domain_iters[d])\n            except StopIteration:\n                domain_iters[d] = iter(loaders_dict[d])\n                imgs, labels, _ = next(domain_iters[d])\n            \n            imgs, labels = imgs.to(device), labels.to(device)\n            \n            optimizer.zero_grad()\n            logits = model(imgs)\n            loss = criterion(logits, labels)\n        \n        # Backward\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        \n        # FIXED: Only track loss (don't accumulate probs/labels for AUC)\n        losses.append(loss.item())\n        mixup_stats['total_batches'] += 1\n        pbar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n    \n    # FIXED: Return simplified metrics (no AUC during training)\n    metrics = {\n        \"loss\": np.mean(losses),\n        \"mixup_ratio\": mixup_stats['mixup_batches'] / mixup_stats['total_batches']\n    }\n    \n    return metrics\n\n@torch.no_grad()\ndef validate(model, loader, criterion, device, thresholds=None):\n    model.eval()\n    losses, all_probs, all_labels = [], [], []\n    \n    for batch in tqdm(loader, desc=\"Val\", leave=False):\n        # Handle both 2-tuple and 3-tuple returns\n        if len(batch) == 3:\n            images, labels, _ = batch\n        else:\n            images, labels = batch\n            \n        images, labels = images.to(device), labels.to(device)\n        logits = model(images)\n        loss = criterion(logits, labels)\n        \n        losses.append(loss.item())\n        all_probs.append(torch.sigmoid(logits).cpu().numpy())\n        all_labels.append(labels.cpu().numpy())\n    \n    probs = np.vstack(all_probs)\n    labels = np.vstack(all_labels)\n    metrics = compute_metrics(labels, probs, thresholds)\n    metrics[\"loss\"] = np.mean(losses)\n    return metrics, probs, labels\n\n# =============================================================================\n# Visualizations (ALL + FIXED Training Curve!)\n# =============================================================================\n\ndef plot_training_curves(history, save_dir):\n    \"\"\"FIXED: Plot training curves (loss + val mAUC only)\"\"\"\n    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n    \n    epochs = range(1, len(history['train_loss']) + 1)\n    \n    # Plot 1: Loss curves\n    axes[0].plot(epochs, history['train_loss'], 'b-', linewidth=2, label='Train Loss', marker='o')\n    axes[0].plot(epochs, history['val_loss'], 'r-', linewidth=2, label='Val Loss', marker='s')\n    axes[0].set_xlabel('Epoch', fontsize=12)\n    axes[0].set_ylabel('Loss', fontsize=12)\n    axes[0].set_title('Training and Validation Loss (Mixup)', fontsize=13, fontweight='bold')\n    axes[0].legend(fontsize=11)\n    axes[0].grid(True, alpha=0.3)\n    \n    # Plot 2: Val mAUC only (can't compute train mAUC with mixed labels)\n    axes[1].plot(epochs, history['val_auc'], 'r-', linewidth=2, label='Val mAUC', marker='s')\n    axes[1].set_xlabel('Epoch', fontsize=12)\n    axes[1].set_ylabel('mAUC', fontsize=12)\n    axes[1].set_title('Validation mAUC (Mixup)', fontsize=13, fontweight='bold')\n    axes[1].legend(fontsize=11)\n    axes[1].grid(True, alpha=0.3)\n    axes[1].set_ylim([0, 1.0])\n    \n    plt.tight_layout()\n    plt.savefig(f'{save_dir}/training_curves.png', dpi=300, bbox_inches='tight')\n    plt.close()\n    print(f\"[INFO] âœ“ Saved training curves\")\n\ndef plot_per_class_roc(labels, probs, test_domain, save_dir):\n    \"\"\"Plot per-class ROC curves (8 subplots)\"\"\"\n    class_names = [\"N\", \"D\", \"G\", \"C\", \"A\", \"H\", \"M\", \"O\"]\n    n_classes = 8\n    \n    fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n    axes = axes.flatten()\n    \n    for i in range(n_classes):\n        if len(np.unique(labels[:, i])) > 1:\n            fpr, tpr, _ = roc_curve(labels[:, i], probs[:, i])\n            auc = roc_auc_score(labels[:, i], probs[:, i])\n            \n            axes[i].plot(fpr, tpr, linewidth=2, label=f'AUC = {auc:.3f}')\n            axes[i].plot([0, 1], [0, 1], 'k--', linewidth=1, alpha=0.5)\n            axes[i].set_xlabel('False Positive Rate', fontsize=10)\n            axes[i].set_ylabel('True Positive Rate', fontsize=10)\n            axes[i].set_title(f'Class {class_names[i]}', fontsize=11, fontweight='bold')\n            axes[i].legend(loc='lower right', fontsize=9)\n            axes[i].grid(True, alpha=0.3)\n        else:\n            axes[i].text(0.5, 0.5, 'Single class\\n(No ROC)', \n                        ha='center', va='center', fontsize=12)\n            axes[i].set_title(f'Class {class_names[i]}', fontsize=11, fontweight='bold')\n    \n    plt.suptitle(f'Per-Class ROC Curves - Test on {test_domain} (Mixup)', \n                 fontsize=14, fontweight='bold', y=0.995)\n    plt.tight_layout()\n    plt.savefig(f'{save_dir}/per_class_roc_curves.png', dpi=300, bbox_inches='tight')\n    plt.close()\n    print(f\"[INFO] âœ“ Saved per-class ROC curves\")\n\ndef plot_macro_roc(labels, probs, test_domain, save_dir):\n    \"\"\"Plot macro-average ROC curve\"\"\"\n    n_classes = 8\n    \n    fig, ax = plt.subplots(figsize=(8, 6))\n    \n    all_fpr, all_tpr = [], []\n    for i in range(n_classes):\n        if len(np.unique(labels[:, i])) > 1:\n            fpr, tpr, _ = roc_curve(labels[:, i], probs[:, i])\n            all_fpr.append(fpr)\n            all_tpr.append(tpr)\n    \n    mean_fpr = np.linspace(0, 1, 100)\n    tprs = [np.interp(mean_fpr, fpr, tpr) for fpr, tpr in zip(all_fpr, all_tpr)]\n    mean_tpr = np.mean(tprs, axis=0)\n    macro_auc = np.trapz(mean_tpr, mean_fpr)\n    \n    ax.plot(mean_fpr, mean_tpr, linewidth=3, \n            label=f'Macro-avg ROC (AUC = {macro_auc:.3f})', color='#2ecc71')\n    ax.plot([0, 1], [0, 1], 'k--', linewidth=2, alpha=0.5, label='Random Classifier')\n    ax.set_xlabel('False Positive Rate', fontsize=12)\n    ax.set_ylabel('True Positive Rate', fontsize=12)\n    ax.set_title(f'Macro-Average ROC Curve - Test on {test_domain} (Mixup)', \n                fontsize=13, fontweight='bold')\n    ax.legend(loc='lower right', fontsize=11)\n    ax.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.savefig(f'{save_dir}/macro_roc_curve.png', dpi=300, bbox_inches='tight')\n    plt.close()\n    print(f\"[INFO] âœ“ Saved macro-average ROC curve\")\n\ndef plot_per_class_metrics(test_metrics, save_dir):\n    \"\"\"Plot per-class AUC and AP bar charts\"\"\"\n    class_names = [\"N\", \"D\", \"G\", \"C\", \"A\", \"H\", \"M\", \"O\"]\n    aucs = test_metrics['per_class_auc']\n    aps = test_metrics['per_class_ap']\n    \n    # Convert nan to 0 for plotting\n    aucs = [auc if not np.isnan(auc) else 0 for auc in aucs]\n    aps = [ap if not np.isnan(ap) else 0 for ap in aps]\n    \n    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n    \n    x = np.arange(len(class_names))\n    \n    # Plot 1: Per-class AUC\n    axes[0].bar(x, aucs, color='#3498db', alpha=0.8, edgecolor='black', linewidth=1.5)\n    axes[0].set_xlabel('Disease Class', fontsize=11)\n    axes[0].set_ylabel('AUC', fontsize=11)\n    axes[0].set_title('Per-Class AUC (Mixup)', fontsize=12, fontweight='bold')\n    axes[0].set_xticks(x)\n    axes[0].set_xticklabels(class_names)\n    axes[0].set_ylim([0, 1.0])\n    axes[0].grid(True, alpha=0.3, axis='y')\n    \n    # Add value labels\n    for i, auc in enumerate(aucs):\n        axes[0].text(i, auc + 0.02, f'{auc:.2f}', ha='center', fontsize=9)\n    \n    # Plot 2: Per-class AP\n    axes[1].bar(x, aps, color='#2ecc71', alpha=0.8, edgecolor='black', linewidth=1.5)\n    axes[1].set_xlabel('Disease Class', fontsize=11)\n    axes[1].set_ylabel('Average Precision', fontsize=11)\n    axes[1].set_title('Per-Class Average Precision (Mixup)', fontsize=12, fontweight='bold')\n    axes[1].set_xticks(x)\n    axes[1].set_xticklabels(class_names)\n    axes[1].set_ylim([0, 1.0])\n    axes[1].grid(True, alpha=0.3, axis='y')\n    \n    # Add value labels\n    for i, ap in enumerate(aps):\n        axes[1].text(i, ap + 0.02, f'{ap:.2f}', ha='center', fontsize=9)\n    \n    plt.tight_layout()\n    plt.savefig(f'{save_dir}/per_class_metrics.png', dpi=300, bbox_inches='tight')\n    plt.close()\n    print(f\"[INFO] âœ“ Saved per-class metrics chart\")\n\n# =============================================================================\n# Main\n# =============================================================================\n\nif __name__ == \"__main__\":\n    set_seed(SEED)\n    os.makedirs(SAVE_DIR, exist_ok=True)\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    print(\"=\"*80)\n    print(f\"LODO FOLD (SWAPPED) with MIXUP DG: Test on {TEST_DOMAIN}\")\n    print(f\"Training on: {TRAIN_DOMAINS}\")\n    print(\"=\"*80)\n    print(f\"Device: {device}\")\n    print(f\"Seed: {SEED}\")\n    print(f\"Mixup Alpha: {MIXUP_ALPHA}\")\n    print(\"=\"*80)\n    \n    # Load data (with domain tracking!)\n    print(\"\\n[INFO] Loading datasets...\")\n    train_datasets = [RetinalDataset(csv, domain_id=i, transform=get_transforms(True)) \n                      for i, csv in enumerate(TRAIN_CSVS)]\n    val_datasets = [RetinalDataset(csv, domain_id=i, transform=get_transforms(False)) \n                    for i, csv in enumerate(VAL_CSVS)]\n    test_dataset = RetinalDataset(TEST_CSV, domain_id=999, transform=get_transforms(False))\n    \n    print(f\"\\n[INFO] Total train: {sum(len(ds) for ds in train_datasets)} images\")\n    print(f\"[INFO] Total val: {sum(len(ds) for ds in val_datasets)} images\")\n    print(f\"[INFO] Test: {len(test_dataset)} images\")\n    \n    # Pos weights\n    pos_weights = calculate_pos_weights(train_datasets).to(device)\n    \n    # DataLoaders (separate per domain for Mixup!)\n    g = torch.Generator().manual_seed(SEED)\n    train_loaders = {\n        i: DataLoader(ds, batch_size=32, shuffle=True, num_workers=4, \n                     pin_memory=True, generator=g)\n        for i, ds in enumerate(train_datasets)\n    }\n    \n    # Combined val loader (no mixup during validation)\n    combined_val = ConcatDataset(val_datasets)\n    val_loader = DataLoader(combined_val, batch_size=32, shuffle=False, \n                           num_workers=4, pin_memory=True)\n    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, \n                            num_workers=4, pin_memory=True)\n    \n    # Model\n    print(\"\\n[INFO] Initializing Mixup model...\")\n    model = MixupMultiLabel().to(device)\n    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weights)\n    optimizer = optim.Adam([\n        {'params': model.backbone.parameters(), 'lr': 1e-4},\n        {'params': model.classifier.parameters(), 'lr': 1e-3}\n    ], weight_decay=1e-4)\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', \n                                                     factor=0.5, patience=3)\n    \n    # Training history (FIXED: no train_auc)\n    history = {\n        'train_loss': [],\n        'val_loss': [],\n        'val_auc': []\n    }\n    \n    # Train\n    best_val_auc, patience_counter = 0.0, 0\n    print(\"\\n[INFO] Training started with Mixup DG...\")\n    print(\"-\"*80)\n    \n    for epoch in range(50):\n        train_metrics = train_epoch_mixup(model, train_loaders, criterion, optimizer, \n                                        device, mixup_alpha=MIXUP_ALPHA)\n        val_metrics, _, _ = validate(model, val_loader, criterion, device)\n        scheduler.step(val_metrics['mAUC'])\n        \n        # Record history (FIXED)\n        history['train_loss'].append(train_metrics['loss'])\n        history['val_loss'].append(val_metrics['loss'])\n        history['val_auc'].append(val_metrics['mAUC'])\n        \n        # Print (FIXED: no train mAUC)\n        print(f\"Epoch {epoch+1:02d} | Train Loss: {train_metrics['loss']:.4f} \"\n              f\"| Val mAUC: {val_metrics['mAUC']:.4f} | Mixup: {train_metrics['mixup_ratio']:.1%}\")\n        \n        if val_metrics['mAUC'] > best_val_auc:\n            best_val_auc = val_metrics['mAUC']\n            torch.save(model.state_dict(), f'{SAVE_DIR}/best_model.pth')\n            print(f\"  âœ“ Saved best model (val mAUC: {best_val_auc:.4f})\")\n            patience_counter = 0\n        else:\n            patience_counter += 1\n        \n        if patience_counter >= 10:\n            print(f\"\\n[INFO] Early stopping at epoch {epoch+1}\")\n            break\n    \n    # Load best & find thresholds\n    print(f\"\\n[INFO] Loading best model...\")\n    model.load_state_dict(torch.load(f'{SAVE_DIR}/best_model.pth', map_location=device))\n    print(f\"[INFO] Best validation mAUC: {best_val_auc:.4f}\")\n    \n    print(f\"\\n[INFO] Finding optimal thresholds on validation...\")\n    _, val_probs, val_labels = validate(model, val_loader, criterion, device)\n    thresholds = find_optimal_thresholds(val_labels, val_probs)\n    \n    class_names = [\"N\", \"D\", \"G\", \"C\", \"A\", \"H\", \"M\", \"O\"]\n    print(\"[INFO] Optimal thresholds:\")\n    for c, t in zip(class_names, thresholds):\n        print(f\"  {c}: {t:.3f}\")\n    \n    # Test\n    print(f\"\\n[INFO] Testing on {TEST_DOMAIN}...\")\n    print(\"-\"*80)\n    test_metrics, test_probs, test_labels = validate(model, test_loader, criterion, \n                                                    device, thresholds)\n    \n    print(\"\\n\" + \"=\"*80)\n    print(f\"TEST RESULTS - {TEST_DOMAIN} (Mixup DG)\")\n    print(\"=\"*80)\n    print(f\"mAUC:      {test_metrics['mAUC']:.4f}\")\n    print(f\"mAP:       {test_metrics['mAP']:.4f}\")\n    print(f\"Macro F1:  {test_metrics['macro_f1']:.4f}\")\n    print(\"=\"*80)\n    \n    # Per-class results\n    print(f\"\\n{'Class':<8} {'AUC':<10} {'AP':<10}\")\n    print(\"-\"*30)\n    for i, cls in enumerate(class_names):\n        auc = test_metrics['per_class_auc'][i]\n        ap = test_metrics['per_class_ap'][i]\n        auc_str = f\"{auc:.4f}\" if not np.isnan(auc) else \"N/A\"\n        ap_str = f\"{ap:.4f}\" if not np.isnan(ap) else \"N/A\"\n        print(f\"{cls:<8} {auc_str:<10} {ap_str:<10}\")\n    print(\"-\"*30)\n    \n    # Save results\n    print(\"\\n[INFO] Saving results...\")\n    results_df = pd.DataFrame([{\n        'method': 'Mixup',\n        'test_domain': TEST_DOMAIN,\n        'train_domains': TRAIN_DOMAINS,\n        'mAUC': test_metrics['mAUC'],\n        'mAP': test_metrics['mAP'],\n        'macro_f1': test_metrics['macro_f1'],\n        'best_val_auc': best_val_auc,\n        'mixup_alpha': MIXUP_ALPHA\n    }])\n    results_df.to_csv(f'{SAVE_DIR}/test_results.csv', index=False)\n    print(f\"[INFO] âœ“ Saved test_results.csv\")\n    \n    # Visualizations\n    print(\"\\n[INFO] Generating visualizations...\")\n    plot_training_curves(history, SAVE_DIR)\n    plot_per_class_roc(test_labels, test_probs, TEST_DOMAIN, SAVE_DIR)\n    plot_macro_roc(test_labels, test_probs, TEST_DOMAIN, SAVE_DIR)\n    plot_per_class_metrics(test_metrics, SAVE_DIR)\n    \n    print(\"\\n\" + \"=\"*80)\n    print(f\"âœ“ MIXUP LODO FOLD (SWAPPED) COMPLETE!\")\n    print(f\"âœ“ Results saved to: {SAVE_DIR}/\")\n    print(\"=\"*80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-17T17:20:00.340881Z","iopub.execute_input":"2026-01-17T17:20:00.341130Z","iopub.status.idle":"2026-01-17T18:34:40.522960Z","shell.execute_reply.started":"2026-01-17T17:20:00.341109Z","shell.execute_reply":"2026-01-17T18:34:40.522197Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nLODO FOLD (SWAPPED) with MIXUP DG: Test on RFMiD_v1\nTraining on: ODIR + RFMiD_v2\n================================================================================\nDevice: cuda\nSeed: 42\nMixup Alpha: 0.2\n================================================================================\n\n[INFO] Loading datasets...\n[INFO] Domain 0: Loaded 7000 valid images\n[INFO] Domain 1: Loaded 507 valid images\n[INFO] Domain 0: Loaded 1000 valid images\n[INFO] Domain 1: Loaded 177 valid images\n[INFO] Domain 999: Loaded 640 valid images\n\n[INFO] Total train: 7507 images\n[INFO] Total val: 1177 images\n[INFO] Test: 640 images\n\n[INFO] Positive class weights (from training domains):\n  N: 2.08\n  D: 2.27\n  G: 15.65\n  C: 15.68\n  A: 21.54\n  H: 26.80\n  M: 19.02\n  O: 2.39\n\n[INFO] Initializing Mixup model...\n","output_type":"stream"},{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/resnet50-11ad3fa6.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-11ad3fa6.pth\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 97.8M/97.8M [00:00<00:00, 193MB/s] \n","output_type":"stream"},{"name":"stdout","text":"\n[INFO] Training started with Mixup DG...\n--------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"                                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 01 | Train Loss: 0.8846 | Val mAUC: 0.7756 | Mixup: 53.9%\n  âœ“ Saved best model (val mAUC: 0.7756)\n","output_type":"stream"},{"name":"stderr","text":"                                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 02 | Train Loss: 0.6882 | Val mAUC: 0.7976 | Mixup: 44.3%\n  âœ“ Saved best model (val mAUC: 0.7976)\n","output_type":"stream"},{"name":"stderr","text":"                                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 03 | Train Loss: 0.6557 | Val mAUC: 0.7994 | Mixup: 51.6%\n  âœ“ Saved best model (val mAUC: 0.7994)\n","output_type":"stream"},{"name":"stderr","text":"                                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 04 | Train Loss: 0.6335 | Val mAUC: 0.8102 | Mixup: 48.4%\n  âœ“ Saved best model (val mAUC: 0.8102)\n","output_type":"stream"},{"name":"stderr","text":"                                                                             ","output_type":"stream"},{"name":"stdout","text":"Epoch 05 | Train Loss: 0.6181 | Val mAUC: 0.7874 | Mixup: 50.2%\n","output_type":"stream"},{"name":"stderr","text":"                                                                             ","output_type":"stream"},{"name":"stdout","text":"Epoch 06 | Train Loss: 0.5893 | Val mAUC: 0.8071 | Mixup: 46.6%\n","output_type":"stream"},{"name":"stderr","text":"                                                                             ","output_type":"stream"},{"name":"stdout","text":"Epoch 07 | Train Loss: 0.5631 | Val mAUC: 0.8098 | Mixup: 53.4%\n","output_type":"stream"},{"name":"stderr","text":"                                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 08 | Train Loss: 0.5209 | Val mAUC: 0.8210 | Mixup: 46.6%\n  âœ“ Saved best model (val mAUC: 0.8210)\n","output_type":"stream"},{"name":"stderr","text":"                                                                             ","output_type":"stream"},{"name":"stdout","text":"Epoch 09 | Train Loss: 0.5715 | Val mAUC: 0.8103 | Mixup: 52.1%\n","output_type":"stream"},{"name":"stderr","text":"                                                                             ","output_type":"stream"},{"name":"stdout","text":"Epoch 10 | Train Loss: 0.4649 | Val mAUC: 0.8028 | Mixup: 52.5%\n","output_type":"stream"},{"name":"stderr","text":"                                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 11 | Train Loss: 0.4902 | Val mAUC: 0.8218 | Mixup: 48.4%\n  âœ“ Saved best model (val mAUC: 0.8218)\n","output_type":"stream"},{"name":"stderr","text":"                                                                             ","output_type":"stream"},{"name":"stdout","text":"Epoch 12 | Train Loss: 0.5312 | Val mAUC: 0.8192 | Mixup: 51.1%\n","output_type":"stream"},{"name":"stderr","text":"                                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 13 | Train Loss: 0.4339 | Val mAUC: 0.8254 | Mixup: 47.5%\n  âœ“ Saved best model (val mAUC: 0.8254)\n","output_type":"stream"},{"name":"stderr","text":"                                                                             ","output_type":"stream"},{"name":"stdout","text":"Epoch 14 | Train Loss: 0.4692 | Val mAUC: 0.8140 | Mixup: 52.1%\n","output_type":"stream"},{"name":"stderr","text":"                                                                             ","output_type":"stream"},{"name":"stdout","text":"Epoch 15 | Train Loss: 0.4483 | Val mAUC: 0.8106 | Mixup: 42.5%\n","output_type":"stream"},{"name":"stderr","text":"                                                                             ","output_type":"stream"},{"name":"stdout","text":"Epoch 16 | Train Loss: 0.4667 | Val mAUC: 0.8170 | Mixup: 46.6%\n","output_type":"stream"},{"name":"stderr","text":"                                                                             ","output_type":"stream"},{"name":"stdout","text":"Epoch 17 | Train Loss: 0.4459 | Val mAUC: 0.7994 | Mixup: 50.7%\n","output_type":"stream"},{"name":"stderr","text":"                                                                             ","output_type":"stream"},{"name":"stdout","text":"Epoch 18 | Train Loss: 0.3810 | Val mAUC: 0.8140 | Mixup: 46.1%\n","output_type":"stream"},{"name":"stderr","text":"                                                                             ","output_type":"stream"},{"name":"stdout","text":"Epoch 19 | Train Loss: 0.3936 | Val mAUC: 0.8141 | Mixup: 53.4%\n","output_type":"stream"},{"name":"stderr","text":"                                                                             ","output_type":"stream"},{"name":"stdout","text":"Epoch 20 | Train Loss: 0.3232 | Val mAUC: 0.8180 | Mixup: 48.4%\n","output_type":"stream"},{"name":"stderr","text":"                                                                             ","output_type":"stream"},{"name":"stdout","text":"Epoch 21 | Train Loss: 0.3411 | Val mAUC: 0.8210 | Mixup: 50.2%\n","output_type":"stream"},{"name":"stderr","text":"                                                                             ","output_type":"stream"},{"name":"stdout","text":"Epoch 22 | Train Loss: 0.3471 | Val mAUC: 0.8226 | Mixup: 54.3%\n","output_type":"stream"},{"name":"stderr","text":"                                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 23 | Train Loss: 0.3151 | Val mAUC: 0.8194 | Mixup: 54.3%\n\n[INFO] Early stopping at epoch 23\n\n[INFO] Loading best model...\n[INFO] Best validation mAUC: 0.8254\n\n[INFO] Finding optimal thresholds on validation...\n","output_type":"stream"},{"name":"stderr","text":"                                                    \r","output_type":"stream"},{"name":"stdout","text":"[INFO] Optimal thresholds:\n  N: 0.420\n  D: 0.160\n  G: 0.670\n  C: 0.680\n  A: 0.390\n  H: 0.640\n  M: 0.910\n  O: 0.380\n\n[INFO] Testing on RFMiD_v1...\n--------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"                                                    \r","output_type":"stream"},{"name":"stdout","text":"\n================================================================================\nTEST RESULTS - RFMiD_v1 (Mixup DG)\n================================================================================\nmAUC:      0.8180\nmAP:       0.4623\nMacro F1:  0.3479\n================================================================================\n\nClass    AUC        AP        \n------------------------------\nN        0.9138     0.7146    \nD        0.8745     0.7095    \nG        0.7128     0.3956    \nC        0.8583     0.6004    \nA        0.8491     0.2560    \nH        0.7887     0.0074    \nM        0.9046     0.4779    \nO        0.6427     0.5374    \n------------------------------\n\n[INFO] Saving results...\n[INFO] âœ“ Saved test_results.csv\n\n[INFO] Generating visualizations...\n[INFO] âœ“ Saved training curves\n[INFO] âœ“ Saved per-class ROC curves\n[INFO] âœ“ Saved macro-average ROC curve\n[INFO] âœ“ Saved per-class metrics chart\n\n================================================================================\nâœ“ MIXUP LODO FOLD (SWAPPED) COMPLETE!\nâœ“ Results saved to: ./results_lodo_mixup/fold_test_RFMiD_v1/\n================================================================================\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}