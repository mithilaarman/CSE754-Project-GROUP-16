{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13811294,"sourceType":"datasetVersion","datasetId":8794300},{"sourceId":13894314,"sourceType":"datasetVersion","datasetId":8852034},{"sourceId":2530487,"sourceType":"datasetVersion","datasetId":1533360}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ============================================================\n# FULL PIPELINE (UPDATED)\n# - Fixes your FileNotFoundError by auto-generating harmonized CSVs\n#   into /kaggle/working/harmonized_labels if they don't exist.\n# - Uses ViT-B/16 (torchvision) instead of ResNet50.\n# - Keeps your Mixup DG training / metrics / plots the same.\n# ============================================================\n\nimport os\nimport random\nfrom typing import Dict, List, Tuple\n\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, ConcatDataset\n\nimport torchvision.transforms as transforms\nimport torchvision.models as models\n\nfrom sklearn.metrics import roc_auc_score, f1_score, roc_curve, average_precision_score\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n\n\n# ============================================================\n# 0) PATHS / CONFIG\n# ============================================================\n\nSEED = 42\n\nSAVE_DIR = './results_lodo_mixup/fold_test_RFMiD_v1'\nTEST_DOMAIN = \"RFMiD_v1\"\nTRAIN_DOMAINS = \"ODIR + RFMiD_v2\"\nMIXUP_ALPHA = 0.2\n\n# Where we will write/read harmonized CSVs (THIS FIXES YOUR ERROR)\nHARMONIZED_DIR = \"/kaggle/working/harmonized_labels\"\n\n# Filenames produced by the harmonization step\nTRAIN_CSVS = [\n    f\"{HARMONIZED_DIR}/ODIR_train.csv\",\n    f\"{HARMONIZED_DIR}/RFMiD_v2_train.csv\",\n]\nVAL_CSVS = [\n    f\"{HARMONIZED_DIR}/ODIR_val.csv\",\n    f\"{HARMONIZED_DIR}/RFMiD_v2_val.csv\",\n]\nTEST_CSV = f\"{HARMONIZED_DIR}/RFMiD_v1_test.csv\"\n\n\n# ============================================================\n# 1) SMALL HELPERS (ROBUST PATH RESOLUTION)\n# ============================================================\n\ndef _pick_existing_path(candidates: List[str], what: str) -> str:\n    \"\"\"Pick the first existing path from candidates, else raise a clear error.\"\"\"\n    for p in candidates:\n        if os.path.exists(p):\n            return p\n    msg = f\"[ERROR] Could not find {what}. Tried:\\n\" + \"\\n\".join(candidates)\n    raise FileNotFoundError(msg)\n\n\n# ============================================================\n# 2) HARMONIZATION: ODIR / RFMiD v1 / RFMiD v2\n# ============================================================\n\ndef harmonize_odir(split: str) -> pd.DataFrame:\n    base_path = \"/kaggle/input/odir-clr/ODIR_CLR\"\n\n    if split == \"train\":\n        img_dir = _pick_existing_path(\n            [\n                f\"{base_path}/Training_Set/train_images\",\n                f\"{base_path}/Training_ Set/train_images\",   # some versions have this\n            ],\n            what=\"ODIR train_images directory\",\n        )\n        label_file = _pick_existing_path(\n            [\n                f\"{base_path}/Training_Set/train_annotation.xlsx\",\n                f\"{base_path}/Training_ Set/train_annotation.xlsx\",\n            ],\n            what=\"ODIR train_annotation.xlsx\",\n        )\n\n    elif split == \"val\":\n        img_dir = _pick_existing_path(\n            [\n                f\"{base_path}/Validation_set/val_images\",\n                f\"{base_path}/Validation_Set/val_images\",\n            ],\n            what=\"ODIR val_images directory\",\n        )\n        label_file = _pick_existing_path(\n            [\n                f\"{base_path}/Validation_set/val_annotation.xlsx\",\n                f\"{base_path}/Validation_Set/val_annotation.xlsx\",\n            ],\n            what=\"ODIR val_annotation.xlsx\",\n        )\n\n    else:\n        img_dir = _pick_existing_path(\n            [\n                f\"{base_path}/Test_Set/test_images\",\n                f\"{base_path}/Test_set/test_images\",\n            ],\n            what=\"ODIR test_images directory\",\n        )\n        label_file = _pick_existing_path(\n            [\n                f\"{base_path}/Test_Set/test_annotation.xlsx\",\n                f\"{base_path}/Test_set/test_annotation.xlsx\",\n            ],\n            what=\"ODIR test_annotation.xlsx\",\n        )\n\n    df = pd.read_excel(label_file)\n\n    label_cols = ['N', 'D', 'G', 'C', 'A', 'H', 'M', 'O']\n    for c in ['ID', 'Left-Fundus', 'Right-Fundus'] + label_cols:\n        if c not in df.columns:\n            raise KeyError(f\"[ERROR] ODIR annotation missing column: {c}\")\n\n    harmonized_rows = []\n    for _, row in df.iterrows():\n        patient_id = row['ID']\n        labels = {col: int(row[col]) for col in label_cols}\n\n        # Keep your original naming convention\n        if pd.notna(row['Left-Fundus']) and str(row['Left-Fundus']).strip():\n            left_path = os.path.join(img_dir, f\"{patient_id}_left.jpg\")\n            harmonized_rows.append({\n                'image_path': left_path,\n                'dataset': 'ODIR',\n                'split': split,\n                'ID': patient_id,\n                **labels\n            })\n\n        if pd.notna(row['Right-Fundus']) and str(row['Right-Fundus']).strip():\n            right_path = os.path.join(img_dir, f\"{patient_id}_right.jpg\")\n            harmonized_rows.append({\n                'image_path': right_path,\n                'dataset': 'ODIR',\n                'split': split,\n                'ID': patient_id,\n                **labels\n            })\n\n    return pd.DataFrame(harmonized_rows)\n\n\ndef harmonize_rfmid_v1(split: str) -> pd.DataFrame:\n    base_path = \"/kaggle/input/retinal-disease-classification\"\n\n    if split == \"train\":\n        img_dir = _pick_existing_path(\n            [\n                f\"{base_path}/Training_Set/Training_Set/Training\",\n                f\"{base_path}/Training_Set/Training\",\n            ],\n            what=\"RFMiD_v1 train image directory\",\n        )\n        label_file = _pick_existing_path(\n            [\n                f\"{base_path}/Training_Set/Training_Set/RFMiD_Training_Labels.csv\",\n                f\"{base_path}/Training_Set/RFMiD_Training_Labels.csv\",\n            ],\n            what=\"RFMiD_v1 RFMiD_Training_Labels.csv\",\n        )\n\n    elif split == \"val\":\n        img_dir = _pick_existing_path(\n            [\n                f\"{base_path}/Evaluation_Set/Evaluation_Set/Validation\",\n                f\"{base_path}/Evaluation_Set/Validation\",\n            ],\n            what=\"RFMiD_v1 val image directory\",\n        )\n        label_file = _pick_existing_path(\n            [\n                f\"{base_path}/Evaluation_Set/Evaluation_Set/RFMiD_Validation_Labels.csv\",\n                f\"{base_path}/Evaluation_Set/RFMiD_Validation_Labels.csv\",\n            ],\n            what=\"RFMiD_v1 RFMiD_Validation_Labels.csv\",\n        )\n\n    else:\n        img_dir = _pick_existing_path(\n            [\n                f\"{base_path}/Test_Set/Test_Set/Test\",\n                f\"{base_path}/Test_Set/Test\",\n            ],\n            what=\"RFMiD_v1 test image directory\",\n        )\n        label_file = _pick_existing_path(\n            [\n                f\"{base_path}/Test_Set/Test_Set/RFMiD_Testing_Labels.csv\",\n                f\"{base_path}/Test_Set/RFMiD_Testing_Labels.csv\",\n            ],\n            what=\"RFMiD_v1 RFMiD_Testing_Labels.csv\",\n        )\n\n    df = pd.read_csv(label_file)\n\n    required = ['ID', 'Disease_Risk', 'DR', 'ODC', 'MH', 'ARMD', 'HR', 'MYA']\n    for c in required:\n        if c not in df.columns:\n            raise KeyError(f\"[ERROR] RFMiD_v1 labels missing column: {c}\")\n\n    all_disease_cols = ['DR', 'ARMD', 'MH', 'DN', 'MYA', 'BRVO', 'TSLN',\n                        'ERM', 'LS', 'MS', 'CSR', 'ODC', 'CRVO', 'TV', 'AH',\n                        'ODP', 'ODE', 'ST', 'AION', 'PT', 'RT', 'RS', 'CRS',\n                        'EDN', 'RPEC', 'MHL', 'RP', 'CWS', 'CB', 'ODPM',\n                        'PRH', 'MNF', 'HR', 'CRAO', 'TD', 'CME', 'PTCR', 'CF',\n                        'VH', 'MCA', 'VS', 'BRAO', 'PLQ', 'HPED', 'CL']\n    all_disease_cols = [c for c in all_disease_cols if c in df.columns]\n\n    harmonized_rows = []\n    for _, row in df.iterrows():\n        image_id = row['ID']\n\n        image_path = None\n        for ext in ['.png', '.jpg', '.jpeg', '.JPG', '.PNG', '.JPEG']:\n            candidate = os.path.join(img_dir, f\"{image_id}{ext}\")\n            if os.path.exists(candidate):\n                image_path = candidate\n                break\n        if image_path is None:\n            image_path = os.path.join(img_dir, str(image_id))\n\n        N = 1 if row['Disease_Risk'] == 0 else 0\n        D = 1 if row.get('DR', 0) == 1 else 0\n        G = 1 if row.get('ODC', 0) == 1 else 0\n        C = 1 if row.get('MH', 0) == 1 else 0\n        A = 1 if row.get('ARMD', 0) == 1 else 0\n        H = 1 if row.get('HR', 0) == 1 else 0\n        M = 1 if row.get('MYA', 0) == 1 else 0\n\n        used_for_mapping = ['DR', 'ODC', 'MH', 'ARMD', 'HR', 'MYA']\n        other_cols = [col for col in all_disease_cols if col not in used_for_mapping]\n        O = 1 if any(row.get(col, 0) == 1 for col in other_cols) else 0\n\n        harmonized_rows.append({\n            'image_path': image_path,\n            'dataset': 'RFMiD_v1',\n            'split': split,\n            'ID': image_id,\n            'N': N, 'D': D, 'G': G, 'C': C, 'A': A, 'H': H, 'M': M, 'O': O\n        })\n\n    return pd.DataFrame(harmonized_rows)\n\n\ndef harmonize_rfmid_v2(split: str) -> pd.DataFrame:\n    base_path = \"/kaggle/input/rdc-version-2/RFDiM2_0\"\n\n    if split == \"train\":\n        img_dir = _pick_existing_path(\n            [\n                f\"{base_path}/Training_set_2/Train_2\",\n                f\"{base_path}/Training_set_2/Train\",\n            ],\n            what=\"RFMiD_v2 train image directory\",\n        )\n        label_file = _pick_existing_path(\n            [\n                f\"{base_path}/Training_set_2/RFMiD_2_Training_labels.csv\",\n            ],\n            what=\"RFMiD_v2 training labels csv\",\n        )\n\n    elif split == \"val\":\n        img_dir = _pick_existing_path(\n            [\n                f\"{base_path}/Validation_set_2/Validation_2\",\n                f\"{base_path}/Validation_set_2/Validation\",\n            ],\n            what=\"RFMiD_v2 val image directory\",\n        )\n        label_file = _pick_existing_path(\n            [\n                f\"{base_path}/Validation_set_2/RFMiD_2_Validation_labels.csv\",\n            ],\n            what=\"RFMiD_v2 validation labels csv\",\n        )\n\n    else:\n        img_dir = _pick_existing_path(\n            [\n                f\"{base_path}/Test_set_2/Test_2\",\n                f\"{base_path}/Test_set_2/Test\",\n            ],\n            what=\"RFMiD_v2 test image directory\",\n        )\n        label_file = _pick_existing_path(\n            [\n                f\"{base_path}/Test_set_2/RFMiD_2_Testing_labels.csv\",\n            ],\n            what=\"RFMiD_v2 testing labels csv\",\n        )\n\n    try:\n        df = pd.read_csv(label_file, encoding='utf-8')\n    except UnicodeDecodeError:\n        df = pd.read_csv(label_file, encoding='latin1')\n\n    df.columns = df.columns.str.strip()\n\n    if 'ID' not in df.columns:\n        raise KeyError(\"[ERROR] RFMiD_v2 labels missing column: ID\")\n\n    potential_disease_cols = ['AH', 'AION', 'ARMD', 'BRVO', 'CB', 'CF', 'CL', 'CME',\n                              'CNV', 'CRAO', 'CRS', 'CRVO', 'CSR', 'CWS', 'CSC', 'DN',\n                              'DR', 'EDN', 'ERM', 'GRT', 'HPED', 'HR', 'LS', 'MCA',\n                              'ME', 'MH', 'MHL', 'MS', 'MYA', 'ODC', 'ODE', 'ODP',\n                              'ON', 'OPDM', 'PRH', 'RD', 'RHL', 'RTR', 'RP', 'RPEC',\n                              'RS', 'RT', 'SOFE', 'ST', 'TD', 'TSLN', 'TV', 'VS',\n                              'HTN', 'IIH', 'WNL']\n    all_disease_cols = [col for col in potential_disease_cols if col in df.columns]\n\n    harmonized_rows = []\n    skipped_count = 0\n    found_count = 0\n\n    for _, row in df.iterrows():\n        try:\n            image_id = int(row['ID'])\n        except Exception:\n            image_id = row['ID']\n\n        image_path = None\n        for ext in ['.jpg', '.JPG', '.png', '.PNG', '.jpeg', '.JPEG']:\n            candidate = os.path.join(img_dir, f\"{image_id}{ext}\")\n            if os.path.exists(candidate):\n                image_path = candidate\n                found_count += 1\n                break\n\n        if image_path is None:\n            skipped_count += 1\n            continue\n\n        wnl = row.get('WNL', 0)\n        N = 1 if wnl == 1 else 0\n        D = 1 if row.get('DR', 0) == 1 else 0\n        G = 1 if row.get('ODC', 0) == 1 else 0\n        C = 1 if row.get('MH', 0) == 1 else 0\n        A = 1 if row.get('ARMD', 0) == 1 else 0\n\n        # Hypertension mapping preference: HTN, fallback HR\n        H = 1 if row.get('HTN', 0) == 1 else 0\n        if H == 0:\n            H = 1 if row.get('HR', 0) == 1 else 0\n\n        M = 1 if row.get('MYA', 0) == 1 else 0\n\n        used_for_mapping = ['DR', 'ODC', 'MH', 'ARMD', 'HTN', 'HR', 'MYA', 'WNL']\n        other_cols = [col for col in all_disease_cols if col not in used_for_mapping]\n        O = 1 if any(row.get(col, 0) == 1 for col in other_cols) else 0\n\n        harmonized_rows.append({\n            'image_path': image_path,\n            'dataset': 'RFMiD_v2',\n            'split': split,\n            'ID': image_id,\n            'N': N, 'D': D, 'G': G, 'C': C, 'A': A, 'H': H, 'M': M, 'O': O\n        })\n\n    if skipped_count > 0:\n        print(f\"[INFO] RFMiD_v2 {split}: Found {found_count}, Skipped {skipped_count}\")\n\n    return pd.DataFrame(harmonized_rows)\n\n\ndef harmonize_all_datasets() -> Dict[str, pd.DataFrame]:\n    results = {}\n    print(\"\\n\" + \"=\"*60)\n    print(\"HARMONIZING DATASETS\")\n    print(\"=\"*60)\n\n    for split in ['train', 'val', 'test']:\n        print(f\"\\nProcessing {split} split...\")\n        results[f'ODIR_{split}'] = harmonize_odir(split)\n        results[f'RFMiD_v1_{split}'] = harmonize_rfmid_v1(split)\n        results[f'RFMiD_v2_{split}'] = harmonize_rfmid_v2(split)\n\n    return results\n\n\ndef save_harmonized_data(harmonized_data: Dict[str, pd.DataFrame], output_dir: str):\n    os.makedirs(output_dir, exist_ok=True)\n\n    print(\"\\n\" + \"=\"*60)\n    print(\"SAVING HARMONIZED DATA\")\n    print(\"=\"*60)\n\n    for key, df in harmonized_data.items():\n        output_path = os.path.join(output_dir, f\"{key}.csv\")\n        df.to_csv(output_path, index=False)\n        print(f\"✅ Saved {key:20s}: {len(df):5d} rows → {output_path}\")\n\n    print(f\"\\n✨ All files saved to: {output_dir}\")\n\n\ndef ensure_harmonized_csvs():\n    \"\"\"If required CSVs don't exist, run harmonization + save to HARMONIZED_DIR.\"\"\"\n    required = TRAIN_CSVS + VAL_CSVS + [TEST_CSV]\n    missing = [p for p in required if not os.path.exists(p)]\n    if not missing:\n        print(f\"[INFO] Found harmonized CSVs in: {HARMONIZED_DIR}\")\n        return\n\n    print(\"\\n[INFO] Harmonized CSVs missing. Will generate them now...\")\n    print(\"[INFO] Missing files:\")\n    for p in missing:\n        print(\"  -\", p)\n\n    harmonized = harmonize_all_datasets()\n    save_harmonized_data(harmonized, output_dir=HARMONIZED_DIR)\n\n    # Final check\n    still_missing = [p for p in required if not os.path.exists(p)]\n    if still_missing:\n        raise FileNotFoundError(\n            \"[ERROR] Tried to generate harmonized CSVs but some are still missing:\\n\"\n            + \"\\n\".join(still_missing)\n        )\n    print(\"[INFO] ✓ Harmonized CSV generation complete.\")\n\n\n# ============================================================\n# 3) TRAINING UTILITIES\n# ============================================================\n\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n\n\nclass RetinalDataset(Dataset):\n    def __init__(self, csv_path, domain_id, transform=None):\n        if not os.path.exists(csv_path):\n            raise FileNotFoundError(f\"[ERROR] CSV not found: {csv_path}\")\n\n        self.data = pd.read_csv(csv_path)\n        self.domain_id = domain_id\n        self.transform = transform\n        self.label_cols = [\"N\", \"D\", \"G\", \"C\", \"A\", \"H\", \"M\", \"O\"]\n\n        dup = self.data.duplicated(subset=[\"image_path\"], keep=\"first\").sum()\n        if dup > 0:\n            print(f\"[WARN] {dup} duplicates found in {csv_path}, keeping first\")\n        self.data = self.data.drop_duplicates(subset=[\"image_path\"]).reset_index(drop=True)\n\n        valid_mask = self.data[\"image_path\"].apply(os.path.exists)\n        missing = (~valid_mask).sum()\n        if missing > 0:\n            print(f\"[WARN] Dropping {missing} missing images from {csv_path}\")\n        self.data = self.data.loc[valid_mask].reset_index(drop=True)\n\n        print(f\"[INFO] Domain {domain_id}: Loaded {len(self.data)} valid images from {os.path.basename(csv_path)}\")\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        row = self.data.iloc[idx]\n        try:\n            image = Image.open(row[\"image_path\"]).convert(\"RGB\")\n        except Exception:\n            image = Image.new(\"RGB\", (224, 224), color=(128, 128, 128))\n\n        labels = torch.tensor(row[self.label_cols].values.astype(\"float32\"))\n        if self.transform:\n            image = self.transform(image)\n        return image, labels, self.domain_id\n\n\ndef calculate_pos_weights(datasets, clip_min=0.5, clip_max=50.0):\n    all_labels = []\n    for ds in datasets:\n        all_labels.append(ds.data[[\"N\", \"D\", \"G\", \"C\", \"A\", \"H\", \"M\", \"O\"]].values)\n    combined = np.vstack(all_labels)\n    pos = combined.sum(axis=0)\n    neg = len(combined) - pos\n    raw = neg / (pos + 1e-5)\n    clipped = np.clip(raw, clip_min, clip_max)\n\n    print(\"\\n[INFO] Positive class weights (from training domains):\")\n    for i, col in enumerate([\"N\", \"D\", \"G\", \"C\", \"A\", \"H\", \"M\", \"O\"]):\n        print(f\"  {col}: {clipped[i]:.2f}\")\n\n    return torch.tensor(clipped, dtype=torch.float32)\n\n\n# ============================================================\n# 4) MODEL: ViT-B/16\n# ============================================================\n\nclass MixupMultiLabelViT(nn.Module):\n    def __init__(self, num_classes=8, dropout=0.3):\n        super().__init__()\n        self.backbone = models.vit_b_16(weights=models.ViT_B_16_Weights.IMAGENET1K_V1)\n\n        hidden_dim = self.backbone.heads.head.in_features\n        self.backbone.heads = nn.Identity()\n\n        self.classifier = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim, 512),\n            nn.ReLU(),\n            nn.BatchNorm1d(512),\n            nn.Dropout(dropout),\n            nn.Linear(512, num_classes),\n        )\n\n    def forward(self, x):\n        feats = self.backbone(x)\n        return self.classifier(feats)\n\n\ndef get_transforms(is_train=False):\n    if is_train:\n        return transforms.Compose([\n            transforms.Resize((256, 256)),\n            transforms.RandomCrop(224),\n            transforms.RandomRotation(10),\n            transforms.ColorJitter(brightness=0.2, contrast=0.2),\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n        ])\n    else:\n        return transforms.Compose([\n            transforms.Resize((224, 224)),\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n        ])\n\n\n# ============================================================\n# 5) METRICS\n# ============================================================\n\ndef compute_metrics(labels, probs, thresholds=None):\n    n_classes = labels.shape[1]\n\n    aucs = []\n    for i in range(n_classes):\n        if len(np.unique(labels[:, i])) > 1:\n            aucs.append(roc_auc_score(labels[:, i], probs[:, i]))\n        else:\n            aucs.append(np.nan)\n\n    aps = []\n    for i in range(n_classes):\n        if len(np.unique(labels[:, i])) > 1:\n            aps.append(average_precision_score(labels[:, i], probs[:, i]))\n        else:\n            aps.append(np.nan)\n\n    if thresholds is None:\n        thresholds = np.full(n_classes, 0.5)\n\n    preds = (probs >= thresholds).astype(int)\n    f1 = f1_score(labels, preds, average=\"macro\", zero_division=0)\n\n    return {\n        \"mAUC\": float(np.nanmean(aucs)),\n        \"mAP\": float(np.nanmean(aps)),\n        \"per_class_auc\": aucs,\n        \"per_class_ap\": aps,\n        \"macro_f1\": float(f1),\n    }\n\n\ndef find_optimal_thresholds(labels, probs):\n    n_classes = labels.shape[1]\n    thresholds = []\n    search_range = np.linspace(0.05, 0.95, 91)\n\n    for i in range(n_classes):\n        best_f1, best_t = 0.0, 0.5\n        if len(np.unique(labels[:, i])) > 1:\n            for t in search_range:\n                preds = (probs[:, i] >= t).astype(int)\n                f1 = f1_score(labels[:, i], preds, zero_division=0)\n                if f1 > best_f1:\n                    best_f1, best_t = f1, t\n        thresholds.append(best_t)\n\n    return np.array(thresholds)\n\n\n# ============================================================\n# 6) MIXUP TRAINING\n# ============================================================\n\ndef train_epoch_mixup(model, loaders_dict, criterion, optimizer, device, mixup_alpha=0.2):\n    model.train()\n    losses = []\n    mixup_stats = {'total_batches': 0, 'mixup_batches': 0}\n\n    domain_iters = {k: iter(v) for k, v in loaders_dict.items()}\n    domain_ids = list(loaders_dict.keys())\n\n    max_batches = max(len(loader) for loader in loaders_dict.values())\n    pbar = tqdm(range(max_batches), desc=\"Train (Mixup)\", leave=False)\n\n    for _ in pbar:\n        if len(domain_ids) >= 2 and np.random.rand() > 0.5:\n            d1, d2 = np.random.choice(domain_ids, size=2, replace=False)\n\n            try:\n                imgs1, labels1, _ = next(domain_iters[d1])\n            except StopIteration:\n                domain_iters[d1] = iter(loaders_dict[d1])\n                imgs1, labels1, _ = next(domain_iters[d1])\n\n            try:\n                imgs2, labels2, _ = next(domain_iters[d2])\n            except StopIteration:\n                domain_iters[d2] = iter(loaders_dict[d2])\n                imgs2, labels2, _ = next(domain_iters[d2])\n\n            min_size = min(imgs1.size(0), imgs2.size(0))\n            imgs1, labels1 = imgs1[:min_size], labels1[:min_size]\n            imgs2, labels2 = imgs2[:min_size], labels2[:min_size]\n\n            lam = np.random.beta(mixup_alpha, mixup_alpha)\n\n            mixed_imgs = lam * imgs1 + (1 - lam) * imgs2\n            mixed_labels = lam * labels1 + (1 - lam) * labels2\n\n            mixed_imgs = mixed_imgs.to(device)\n            mixed_labels = mixed_labels.to(device)\n\n            optimizer.zero_grad()\n            logits = model(mixed_imgs)\n            loss = criterion(logits, mixed_labels)\n            mixup_stats['mixup_batches'] += 1\n\n        else:\n            d = np.random.choice(domain_ids)\n            try:\n                imgs, labels, _ = next(domain_iters[d])\n            except StopIteration:\n                domain_iters[d] = iter(loaders_dict[d])\n                imgs, labels, _ = next(domain_iters[d])\n\n            imgs, labels = imgs.to(device), labels.to(device)\n            optimizer.zero_grad()\n            logits = model(imgs)\n            loss = criterion(logits, labels)\n\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n\n        losses.append(loss.item())\n        mixup_stats['total_batches'] += 1\n        pbar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n\n    return {\n        \"loss\": float(np.mean(losses)),\n        \"mixup_ratio\": float(mixup_stats['mixup_batches'] / mixup_stats['total_batches'])\n    }\n\n\n@torch.no_grad()\ndef validate(model, loader, criterion, device, thresholds=None):\n    model.eval()\n    losses, all_probs, all_labels = [], [], []\n\n    for batch in tqdm(loader, desc=\"Val\", leave=False):\n        if len(batch) == 3:\n            images, labels, _ = batch\n        else:\n            images, labels = batch\n\n        images, labels = images.to(device), labels.to(device)\n        logits = model(images)\n        loss = criterion(logits, labels)\n\n        losses.append(loss.item())\n        all_probs.append(torch.sigmoid(logits).cpu().numpy())\n        all_labels.append(labels.cpu().numpy())\n\n    probs = np.vstack(all_probs)\n    labels = np.vstack(all_labels)\n    metrics = compute_metrics(labels, probs, thresholds)\n    metrics[\"loss\"] = float(np.mean(losses))\n    return metrics, probs, labels\n\n\n# ============================================================\n# 7) PLOTS\n# ============================================================\n\ndef plot_training_curves(history, save_dir):\n    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n    epochs = range(1, len(history['train_loss']) + 1)\n\n    axes[0].plot(epochs, history['train_loss'], linewidth=2, label='Train Loss', marker='o')\n    axes[0].plot(epochs, history['val_loss'], linewidth=2, label='Val Loss', marker='s')\n    axes[0].set_xlabel('Epoch')\n    axes[0].set_ylabel('Loss')\n    axes[0].set_title('Training and Validation Loss (Mixup)')\n    axes[0].legend()\n    axes[0].grid(True, alpha=0.3)\n\n    axes[1].plot(epochs, history['val_auc'], linewidth=2, label='Val mAUC', marker='s')\n    axes[1].set_xlabel('Epoch')\n    axes[1].set_ylabel('mAUC')\n    axes[1].set_title('Validation mAUC (Mixup)')\n    axes[1].legend()\n    axes[1].grid(True, alpha=0.3)\n    axes[1].set_ylim([0, 1.0])\n\n    plt.tight_layout()\n    os.makedirs(save_dir, exist_ok=True)\n    plt.savefig(f'{save_dir}/training_curves.png', dpi=300, bbox_inches='tight')\n    plt.close()\n    print(\"[INFO] ✓ Saved training curves\")\n\n\ndef plot_per_class_roc(labels, probs, test_domain, save_dir):\n    class_names = [\"N\", \"D\", \"G\", \"C\", \"A\", \"H\", \"M\", \"O\"]\n    n_classes = 8\n\n    fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n    axes = axes.flatten()\n\n    for i in range(n_classes):\n        if len(np.unique(labels[:, i])) > 1:\n            fpr, tpr, _ = roc_curve(labels[:, i], probs[:, i])\n            auc = roc_auc_score(labels[:, i], probs[:, i])\n            axes[i].plot(fpr, tpr, linewidth=2, label=f'AUC = {auc:.3f}')\n            axes[i].plot([0, 1], [0, 1], 'k--', linewidth=1, alpha=0.5)\n            axes[i].set_title(f'Class {class_names[i]}')\n            axes[i].set_xlabel('FPR')\n            axes[i].set_ylabel('TPR')\n            axes[i].legend(loc='lower right')\n            axes[i].grid(True, alpha=0.3)\n        else:\n            axes[i].text(0.5, 0.5, 'Single class\\n(No ROC)', ha='center', va='center')\n            axes[i].set_title(f'Class {class_names[i]}')\n\n    plt.suptitle(f'Per-Class ROC Curves - Test on {test_domain} (Mixup)', y=0.995)\n    plt.tight_layout()\n    os.makedirs(save_dir, exist_ok=True)\n    plt.savefig(f'{save_dir}/per_class_roc_curves.png', dpi=300, bbox_inches='tight')\n    plt.close()\n    print(\"[INFO] ✓ Saved per-class ROC curves\")\n\n\ndef plot_macro_roc(labels, probs, test_domain, save_dir):\n    n_classes = 8\n    fig, ax = plt.subplots(figsize=(8, 6))\n\n    all_fpr, all_tpr = [], []\n    for i in range(n_classes):\n        if len(np.unique(labels[:, i])) > 1:\n            fpr, tpr, _ = roc_curve(labels[:, i], probs[:, i])\n            all_fpr.append(fpr)\n            all_tpr.append(tpr)\n\n    if len(all_fpr) == 0:\n        print(\"[WARN] Cannot plot macro ROC: no class has both positives & negatives.\")\n        return\n\n    mean_fpr = np.linspace(0, 1, 100)\n    tprs = [np.interp(mean_fpr, fpr, tpr) for fpr, tpr in zip(all_fpr, all_tpr)]\n    mean_tpr = np.mean(tprs, axis=0)\n    macro_auc = np.trapz(mean_tpr, mean_fpr)\n\n    ax.plot(mean_fpr, mean_tpr, linewidth=3, label=f'Macro-avg ROC (AUC = {macro_auc:.3f})')\n    ax.plot([0, 1], [0, 1], 'k--', linewidth=2, alpha=0.5, label='Random')\n    ax.set_xlabel('FPR')\n    ax.set_ylabel('TPR')\n    ax.set_title(f'Macro-Average ROC - Test on {test_domain} (Mixup)')\n    ax.legend(loc='lower right')\n    ax.grid(True, alpha=0.3)\n\n    plt.tight_layout()\n    os.makedirs(save_dir, exist_ok=True)\n    plt.savefig(f'{save_dir}/macro_roc_curve.png', dpi=300, bbox_inches='tight')\n    plt.close()\n    print(\"[INFO] ✓ Saved macro-average ROC curve\")\n\n\ndef plot_per_class_metrics(test_metrics, save_dir):\n    class_names = [\"N\", \"D\", \"G\", \"C\", \"A\", \"H\", \"M\", \"O\"]\n    aucs = test_metrics['per_class_auc']\n    aps = test_metrics['per_class_ap']\n\n    aucs = [a if not np.isnan(a) else 0 for a in aucs]\n    aps = [a if not np.isnan(a) else 0 for a in aps]\n\n    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n    x = np.arange(len(class_names))\n\n    axes[0].bar(x, aucs, alpha=0.85, edgecolor='black', linewidth=1.2)\n    axes[0].set_title('Per-Class AUC (Mixup)')\n    axes[0].set_xticks(x)\n    axes[0].set_xticklabels(class_names)\n    axes[0].set_ylim([0, 1.0])\n    axes[0].grid(True, alpha=0.3, axis='y')\n\n    axes[1].bar(x, aps, alpha=0.85, edgecolor='black', linewidth=1.2)\n    axes[1].set_title('Per-Class AP (Mixup)')\n    axes[1].set_xticks(x)\n    axes[1].set_xticklabels(class_names)\n    axes[1].set_ylim([0, 1.0])\n    axes[1].grid(True, alpha=0.3, axis='y')\n\n    plt.tight_layout()\n    os.makedirs(save_dir, exist_ok=True)\n    plt.savefig(f'{save_dir}/per_class_metrics.png', dpi=300, bbox_inches='tight')\n    plt.close()\n    print(\"[INFO] ✓ Saved per-class metrics chart\")\n\n\n# ============================================================\n# 8) MAIN\n# ============================================================\n\nif __name__ == \"__main__\":\n    set_seed(SEED)\n    os.makedirs(SAVE_DIR, exist_ok=True)\n\n    print(\"=\"*80)\n    print(f\"LODO FOLD (SWAPPED) with MIXUP DG: Test on {TEST_DOMAIN}\")\n    print(f\"Training on: {TRAIN_DOMAINS}\")\n    print(\"=\"*80)\n\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Device: {device}\")\n    print(f\"Seed: {SEED}\")\n    print(f\"Mixup Alpha: {MIXUP_ALPHA}\")\n    print(\"=\"*80)\n\n    # ---- FIX: ensure harmonized CSVs exist BEFORE loading datasets\n    ensure_harmonized_csvs()\n\n    print(\"\\n[INFO] Loading datasets...\")\n    train_datasets = [RetinalDataset(csv, domain_id=i, transform=get_transforms(True))\n                      for i, csv in enumerate(TRAIN_CSVS)]\n    val_datasets = [RetinalDataset(csv, domain_id=i, transform=get_transforms(False))\n                    for i, csv in enumerate(VAL_CSVS)]\n    test_dataset = RetinalDataset(TEST_CSV, domain_id=999, transform=get_transforms(False))\n\n    print(f\"\\n[INFO] Total train: {sum(len(ds) for ds in train_datasets)} images\")\n    print(f\"[INFO] Total val: {sum(len(ds) for ds in val_datasets)} images\")\n    print(f\"[INFO] Test: {len(test_dataset)} images\")\n\n    pos_weights = calculate_pos_weights(train_datasets).to(device)\n\n    g = torch.Generator().manual_seed(SEED)\n    train_loaders = {\n        i: DataLoader(ds, batch_size=32, shuffle=True, num_workers=4,\n                      pin_memory=True, generator=g)\n        for i, ds in enumerate(train_datasets)\n    }\n\n    combined_val = ConcatDataset(val_datasets)\n    val_loader = DataLoader(combined_val, batch_size=32, shuffle=False,\n                            num_workers=4, pin_memory=True)\n    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False,\n                             num_workers=4, pin_memory=True)\n\n    print(\"\\n[INFO] Initializing ViT-B/16 model...\")\n    model = MixupMultiLabelViT().to(device)\n\n    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weights)\n\n    # ViT usually prefers smaller LRs than ResNet\n    optimizer = optim.Adam([\n        {'params': model.backbone.parameters(), 'lr': 1e-5},\n        {'params': model.classifier.parameters(), 'lr': 1e-4},\n    ], weight_decay=1e-4)\n\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, mode='max', factor=0.5, patience=3\n    )\n\n    history = {'train_loss': [], 'val_loss': [], 'val_auc': []}\n\n    best_val_auc, patience_counter = 0.0, 0\n    print(\"\\n[INFO] Training started with Mixup DG (ViT-B/16)...\")\n    print(\"-\"*80)\n\n    for epoch in range(50):\n        train_metrics = train_epoch_mixup(\n            model, train_loaders, criterion, optimizer, device, mixup_alpha=MIXUP_ALPHA\n        )\n        val_metrics, _, _ = validate(model, val_loader, criterion, device)\n        scheduler.step(val_metrics['mAUC'])\n\n        history['train_loss'].append(train_metrics['loss'])\n        history['val_loss'].append(val_metrics['loss'])\n        history['val_auc'].append(val_metrics['mAUC'])\n\n        print(f\"Epoch {epoch+1:02d} | Train Loss: {train_metrics['loss']:.4f} \"\n              f\"| Val mAUC: {val_metrics['mAUC']:.4f} | Mixup: {train_metrics['mixup_ratio']:.1%}\")\n\n        if val_metrics['mAUC'] > best_val_auc:\n            best_val_auc = val_metrics['mAUC']\n            torch.save(model.state_dict(), f'{SAVE_DIR}/best_model.pth')\n            print(f\"  ✓ Saved best model (val mAUC: {best_val_auc:.4f})\")\n            patience_counter = 0\n        else:\n            patience_counter += 1\n\n        if patience_counter >= 10:\n            print(f\"\\n[INFO] Early stopping at epoch {epoch+1}\")\n            break\n\n    print(f\"\\n[INFO] Loading best model...\")\n    model.load_state_dict(torch.load(f'{SAVE_DIR}/best_model.pth', map_location=device))\n    print(f\"[INFO] Best validation mAUC: {best_val_auc:.4f}\")\n\n    print(f\"\\n[INFO] Finding optimal thresholds on validation...\")\n    _, val_probs, val_labels = validate(model, val_loader, criterion, device)\n    thresholds = find_optimal_thresholds(val_labels, val_probs)\n\n    class_names = [\"N\", \"D\", \"G\", \"C\", \"A\", \"H\", \"M\", \"O\"]\n    print(\"[INFO] Optimal thresholds:\")\n    for c, t in zip(class_names, thresholds):\n        print(f\"  {c}: {t:.3f}\")\n\n    print(f\"\\n[INFO] Testing on {TEST_DOMAIN}...\")\n    print(\"-\"*80)\n    test_metrics, test_probs, test_labels = validate(\n        model, test_loader, criterion, device, thresholds\n    )\n\n    print(\"\\n\" + \"=\"*80)\n    print(f\"TEST RESULTS - {TEST_DOMAIN} (Mixup DG, ViT-B/16)\")\n    print(\"=\"*80)\n    print(f\"mAUC:      {test_metrics['mAUC']:.4f}\")\n    print(f\"mAP:       {test_metrics['mAP']:.4f}\")\n    print(f\"Macro F1:  {test_metrics['macro_f1']:.4f}\")\n    print(\"=\"*80)\n\n    print(f\"\\n{'Class':<8} {'AUC':<10} {'AP':<10}\")\n    print(\"-\"*30)\n    for i, cls in enumerate(class_names):\n        auc = test_metrics['per_class_auc'][i]\n        ap = test_metrics['per_class_ap'][i]\n        auc_str = f\"{auc:.4f}\" if not np.isnan(auc) else \"N/A\"\n        ap_str = f\"{ap:.4f}\" if not np.isnan(ap) else \"N/A\"\n        print(f\"{cls:<8} {auc_str:<10} {ap_str:<10}\")\n    print(\"-\"*30)\n\n    print(\"\\n[INFO] Saving results...\")\n    results_df = pd.DataFrame([{\n        'method': 'Mixup',\n        'backbone': 'vit_b_16',\n        'test_domain': TEST_DOMAIN,\n        'train_domains': TRAIN_DOMAINS,\n        'mAUC': test_metrics['mAUC'],\n        'mAP': test_metrics['mAP'],\n        'macro_f1': test_metrics['macro_f1'],\n        'best_val_auc': best_val_auc,\n        'mixup_alpha': MIXUP_ALPHA\n    }])\n    results_df.to_csv(f'{SAVE_DIR}/test_results.csv', index=False)\n    print(f\"[INFO] ✓ Saved test_results.csv\")\n\n    print(\"\\n[INFO] Generating visualizations...\")\n    plot_training_curves(history, SAVE_DIR)\n    plot_per_class_roc(test_labels, test_probs, TEST_DOMAIN, SAVE_DIR)\n    plot_macro_roc(test_labels, test_probs, TEST_DOMAIN, SAVE_DIR)\n    plot_per_class_metrics(test_metrics, SAVE_DIR)\n\n    print(\"\\n\" + \"=\"*80)\n    print(\"✓ MIXUP LODO FOLD (SWAPPED) COMPLETE! (ViT-B/16)\")\n    print(f\"✓ Results saved to: {SAVE_DIR}/\")\n    print(\"=\"*80)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-18T03:03:56.078693Z","iopub.execute_input":"2026-01-18T03:03:56.078950Z","iopub.status.idle":"2026-01-18T04:47:07.423290Z","shell.execute_reply.started":"2026-01-18T03:03:56.078926Z","shell.execute_reply":"2026-01-18T04:47:07.422492Z"}},"outputs":[{"name":"stdout","text":"================================================================================\nLODO FOLD (SWAPPED) with MIXUP DG: Test on RFMiD_v1\nTraining on: ODIR + RFMiD_v2\n================================================================================\nDevice: cuda\nSeed: 42\nMixup Alpha: 0.2\n================================================================================\n\n[INFO] Harmonized CSVs missing. Will generate them now...\n[INFO] Missing files:\n  - /kaggle/working/harmonized_labels/ODIR_train.csv\n  - /kaggle/working/harmonized_labels/RFMiD_v2_train.csv\n  - /kaggle/working/harmonized_labels/ODIR_val.csv\n  - /kaggle/working/harmonized_labels/RFMiD_v2_val.csv\n  - /kaggle/working/harmonized_labels/RFMiD_v1_test.csv\n\n============================================================\nHARMONIZING DATASETS\n============================================================\n\nProcessing train split...\n[INFO] RFMiD_v2 train: Found 507, Skipped 2\n\nProcessing val split...\n\nProcessing test split...\n[INFO] RFMiD_v2 test: Found 170, Skipped 4\n\n============================================================\nSAVING HARMONIZED DATA\n============================================================\n✅ Saved ODIR_train          :  7000 rows → /kaggle/working/harmonized_labels/ODIR_train.csv\n✅ Saved RFMiD_v1_train      :  1920 rows → /kaggle/working/harmonized_labels/RFMiD_v1_train.csv\n✅ Saved RFMiD_v2_train      :   507 rows → /kaggle/working/harmonized_labels/RFMiD_v2_train.csv\n✅ Saved ODIR_val            :  1000 rows → /kaggle/working/harmonized_labels/ODIR_val.csv\n✅ Saved RFMiD_v1_val        :   640 rows → /kaggle/working/harmonized_labels/RFMiD_v1_val.csv\n✅ Saved RFMiD_v2_val        :   177 rows → /kaggle/working/harmonized_labels/RFMiD_v2_val.csv\n✅ Saved ODIR_test           :  2000 rows → /kaggle/working/harmonized_labels/ODIR_test.csv\n✅ Saved RFMiD_v1_test       :   640 rows → /kaggle/working/harmonized_labels/RFMiD_v1_test.csv\n✅ Saved RFMiD_v2_test       :   170 rows → /kaggle/working/harmonized_labels/RFMiD_v2_test.csv\n\n✨ All files saved to: /kaggle/working/harmonized_labels\n[INFO] ✓ Harmonized CSV generation complete.\n\n[INFO] Loading datasets...\n[INFO] Domain 0: Loaded 7000 valid images from ODIR_train.csv\n[INFO] Domain 1: Loaded 507 valid images from RFMiD_v2_train.csv\n[INFO] Domain 0: Loaded 1000 valid images from ODIR_val.csv\n[INFO] Domain 1: Loaded 177 valid images from RFMiD_v2_val.csv\n[INFO] Domain 999: Loaded 640 valid images from RFMiD_v1_test.csv\n\n[INFO] Total train: 7507 images\n[INFO] Total val: 1177 images\n[INFO] Test: 640 images\n\n[INFO] Positive class weights (from training domains):\n  N: 2.08\n  D: 2.27\n  G: 15.65\n  C: 15.68\n  A: 21.54\n  H: 26.80\n  M: 19.02\n  O: 2.39\n\n[INFO] Initializing ViT-B/16 model...\nDownloading: \"https://download.pytorch.org/models/vit_b_16-c867db91.pth\" to /root/.cache/torch/hub/checkpoints/vit_b_16-c867db91.pth\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 330M/330M [00:01<00:00, 180MB/s]  \n","output_type":"stream"},{"name":"stdout","text":"\n[INFO] Training started with Mixup DG (ViT-B/16)...\n--------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"                                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 01 | Train Loss: 0.9637 | Val mAUC: 0.8046 | Mixup: 53.9%\n  ✓ Saved best model (val mAUC: 0.8046)\n","output_type":"stream"},{"name":"stderr","text":"                                                                             ","output_type":"stream"},{"name":"stdout","text":"Epoch 02 | Train Loss: 0.8047 | Val mAUC: 0.7935 | Mixup: 44.3%\n","output_type":"stream"},{"name":"stderr","text":"                                                                             ","output_type":"stream"},{"name":"stdout","text":"Epoch 03 | Train Loss: 0.7324 | Val mAUC: 0.8004 | Mixup: 51.6%\n","output_type":"stream"},{"name":"stderr","text":"                                                                             ","output_type":"stream"},{"name":"stdout","text":"Epoch 04 | Train Loss: 0.6861 | Val mAUC: 0.8035 | Mixup: 48.4%\n","output_type":"stream"},{"name":"stderr","text":"                                                                             ","output_type":"stream"},{"name":"stdout","text":"Epoch 05 | Train Loss: 0.6535 | Val mAUC: 0.8035 | Mixup: 50.2%\n","output_type":"stream"},{"name":"stderr","text":"                                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 06 | Train Loss: 0.6066 | Val mAUC: 0.8174 | Mixup: 46.6%\n  ✓ Saved best model (val mAUC: 0.8174)\n","output_type":"stream"},{"name":"stderr","text":"                                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 07 | Train Loss: 0.5684 | Val mAUC: 0.8198 | Mixup: 53.4%\n  ✓ Saved best model (val mAUC: 0.8198)\n","output_type":"stream"},{"name":"stderr","text":"                                                                             ","output_type":"stream"},{"name":"stdout","text":"Epoch 08 | Train Loss: 0.5231 | Val mAUC: 0.8138 | Mixup: 46.6%\n","output_type":"stream"},{"name":"stderr","text":"                                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 09 | Train Loss: 0.5684 | Val mAUC: 0.8223 | Mixup: 52.1%\n  ✓ Saved best model (val mAUC: 0.8223)\n","output_type":"stream"},{"name":"stderr","text":"                                                                             ","output_type":"stream"},{"name":"stdout","text":"Epoch 10 | Train Loss: 0.4889 | Val mAUC: 0.8175 | Mixup: 52.5%\n","output_type":"stream"},{"name":"stderr","text":"                                                                             ","output_type":"stream"},{"name":"stdout","text":"Epoch 11 | Train Loss: 0.5202 | Val mAUC: 0.8165 | Mixup: 48.4%\n","output_type":"stream"},{"name":"stderr","text":"                                                                             ","output_type":"stream"},{"name":"stdout","text":"Epoch 12 | Train Loss: 0.5410 | Val mAUC: 0.8152 | Mixup: 51.1%\n","output_type":"stream"},{"name":"stderr","text":"                                                                             ","output_type":"stream"},{"name":"stdout","text":"Epoch 13 | Train Loss: 0.4783 | Val mAUC: 0.8133 | Mixup: 47.5%\n","output_type":"stream"},{"name":"stderr","text":"                                                                             ","output_type":"stream"},{"name":"stdout","text":"Epoch 14 | Train Loss: 0.5026 | Val mAUC: 0.8125 | Mixup: 52.1%\n","output_type":"stream"},{"name":"stderr","text":"                                                                             ","output_type":"stream"},{"name":"stdout","text":"Epoch 15 | Train Loss: 0.4723 | Val mAUC: 0.8161 | Mixup: 42.5%\n","output_type":"stream"},{"name":"stderr","text":"                                                                             ","output_type":"stream"},{"name":"stdout","text":"Epoch 16 | Train Loss: 0.5063 | Val mAUC: 0.8120 | Mixup: 46.6%\n","output_type":"stream"},{"name":"stderr","text":"                                                                             ","output_type":"stream"},{"name":"stdout","text":"Epoch 17 | Train Loss: 0.4978 | Val mAUC: 0.8166 | Mixup: 50.7%\n","output_type":"stream"},{"name":"stderr","text":"                                                                             ","output_type":"stream"},{"name":"stdout","text":"Epoch 18 | Train Loss: 0.4467 | Val mAUC: 0.8138 | Mixup: 46.1%\n","output_type":"stream"},{"name":"stderr","text":"                                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 19 | Train Loss: 0.5087 | Val mAUC: 0.8172 | Mixup: 53.4%\n\n[INFO] Early stopping at epoch 19\n\n[INFO] Loading best model...\n[INFO] Best validation mAUC: 0.8223\n\n[INFO] Finding optimal thresholds on validation...\n","output_type":"stream"},{"name":"stderr","text":"                                                    \r","output_type":"stream"},{"name":"stdout","text":"[INFO] Optimal thresholds:\n  N: 0.500\n  D: 0.220\n  G: 0.550\n  C: 0.910\n  A: 0.730\n  H: 0.640\n  M: 0.820\n  O: 0.480\n\n[INFO] Testing on RFMiD_v1...\n--------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"                                                    \r","output_type":"stream"},{"name":"stdout","text":"\n================================================================================\nTEST RESULTS - RFMiD_v1 (Mixup DG, ViT-B/16)\n================================================================================\nmAUC:      0.8293\nmAP:       0.4709\nMacro F1:  0.4102\n================================================================================\n\nClass    AUC        AP        \n------------------------------\nN        0.8856     0.6902    \nD        0.8261     0.5735    \nG        0.7426     0.3669    \nC        0.8926     0.7821    \nA        0.8343     0.2636    \nH        0.9061     0.0164    \nM        0.9274     0.5711    \nO        0.6195     0.5039    \n------------------------------\n\n[INFO] Saving results...\n[INFO] ✓ Saved test_results.csv\n\n[INFO] Generating visualizations...\n[INFO] ✓ Saved training curves\n[INFO] ✓ Saved per-class ROC curves\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_55/147332802.py:787: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n  macro_auc = np.trapz(mean_tpr, mean_fpr)\n","output_type":"stream"},{"name":"stdout","text":"[INFO] ✓ Saved macro-average ROC curve\n[INFO] ✓ Saved per-class metrics chart\n\n================================================================================\n✓ MIXUP LODO FOLD (SWAPPED) COMPLETE! (ViT-B/16)\n✓ Results saved to: ./results_lodo_mixup/fold_test_RFMiD_v1/\n================================================================================\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}