{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":2530487,"sourceType":"datasetVersion","datasetId":1533360},{"sourceId":13894314,"sourceType":"datasetVersion","datasetId":8852034},{"sourceId":13811294,"sourceType":"datasetVersion","datasetId":8794300}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ============================================================\n# FULL SCRIPT (FIXED): Auto-generate missing harmonized CSVs\n# + Swin Transformer Tiny (swin_t) backbone\n#\n# This fixes your error:\n# FileNotFoundError: /kaggle/working/harmonized_labels/ODIR_train.csv\n# by generating /kaggle/working/harmonized_labels/*.csv automatically\n# if they don't exist.\n# ============================================================\n\nimport os\nimport random\nfrom typing import Dict\n\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, ConcatDataset\n\nimport torchvision.transforms as transforms\nimport torchvision.models as models\n\nfrom sklearn.metrics import roc_auc_score, f1_score, roc_curve, average_precision_score\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n\n\n# ============================================================\n# 1) HARMONIZATION\n# ============================================================\n\ndef harmonize_odir(split: str) -> pd.DataFrame:\n    base_path = \"/kaggle/input/odir-clr/ODIR_CLR\"\n\n    if split == \"train\":\n        img_dir = f\"{base_path}/Training_ Set/train_images\"\n        label_file = f\"{base_path}/Training_ Set/train_annotation.xlsx\"\n    elif split == \"val\":\n        img_dir = f\"{base_path}/Validation_set/val_images\"\n        label_file = f\"{base_path}/Validation_set/val_annotation.xlsx\"\n    else:\n        img_dir = f\"{base_path}/Test_Set/test_images\"\n        label_file = f\"{base_path}/Test_Set/test_annotation.xlsx\"\n\n    df = pd.read_excel(label_file)\n    label_cols = ['N', 'D', 'G', 'C', 'A', 'H', 'M', 'O']\n    harmonized_rows = []\n\n    for _, row in df.iterrows():\n        patient_id = row['ID']\n        labels = {col: int(row[col]) for col in label_cols}\n\n        if pd.notna(row.get('Left-Fundus', np.nan)) and str(row['Left-Fundus']).strip():\n            left_path = os.path.join(img_dir, f\"{patient_id}_left.jpg\")\n            harmonized_rows.append({\n                'image_path': left_path,\n                'dataset': 'ODIR',\n                'split': split,\n                'ID': patient_id,\n                **labels\n            })\n\n        if pd.notna(row.get('Right-Fundus', np.nan)) and str(row['Right-Fundus']).strip():\n            right_path = os.path.join(img_dir, f\"{patient_id}_right.jpg\")\n            harmonized_rows.append({\n                'image_path': right_path,\n                'dataset': 'ODIR',\n                'split': split,\n                'ID': patient_id,\n                **labels\n            })\n\n    return pd.DataFrame(harmonized_rows)\n\n\ndef harmonize_rfmid_v1(split: str) -> pd.DataFrame:\n    base_path = \"/kaggle/input/retinal-disease-classification\"\n\n    if split == \"train\":\n        img_dir = f\"{base_path}/Training_Set/Training_Set/Training\"\n        label_file = f\"{base_path}/Training_Set/Training_Set/RFMiD_Training_Labels.csv\"\n    elif split == \"val\":\n        img_dir = f\"{base_path}/Evaluation_Set/Evaluation_Set/Validation\"\n        label_file = f\"{base_path}/Evaluation_Set/Evaluation_Set/RFMiD_Validation_Labels.csv\"\n    else:\n        img_dir = f\"{base_path}/Test_Set/Test_Set/Test\"\n        label_file = f\"{base_path}/Test_Set/Test_Set/RFMiD_Testing_Labels.csv\"\n\n    df = pd.read_csv(label_file)\n\n    all_disease_cols = ['DR', 'ARMD', 'MH', 'DN', 'MYA', 'BRVO', 'TSLN',\n                        'ERM', 'LS', 'MS', 'CSR', 'ODC', 'CRVO', 'TV', 'AH',\n                        'ODP', 'ODE', 'ST', 'AION', 'PT', 'RT', 'RS', 'CRS',\n                        'EDN', 'RPEC', 'MHL', 'RP', 'CWS', 'CB', 'ODPM',\n                        'PRH', 'MNF', 'HR', 'CRAO', 'TD', 'CME', 'PTCR', 'CF',\n                        'VH', 'MCA', 'VS', 'BRAO', 'PLQ', 'HPED', 'CL']\n\n    harmonized_rows = []\n\n    for _, row in df.iterrows():\n        image_id = row['ID']\n\n        # Try common extensions\n        image_path = None\n        for ext in ['.png', '.jpg', '.jpeg', '.JPG', '.PNG', '.JPEG']:\n            candidate = os.path.join(img_dir, f\"{image_id}{ext}\")\n            if os.path.exists(candidate):\n                image_path = candidate\n                break\n        if image_path is None:\n            # last fallback (some datasets store exact filename)\n            image_path = os.path.join(img_dir, str(image_id))\n\n        # Mapping\n        N = 1 if row['Disease_Risk'] == 0 else 0\n        D = 1 if row.get('DR', 0) == 1 else 0\n        G = 1 if row.get('ODC', 0) == 1 else 0\n        C = 1 if row.get('MH', 0) == 1 else 0\n        A = 1 if row.get('ARMD', 0) == 1 else 0\n        H = 1 if row.get('HR', 0) == 1 else 0\n        M = 1 if row.get('MYA', 0) == 1 else 0\n\n        used_for_mapping = ['DR', 'ODC', 'MH', 'ARMD', 'HR', 'MYA']\n        other_cols = [col for col in all_disease_cols if col not in used_for_mapping]\n        O = 1 if any(row.get(col, 0) == 1 for col in other_cols) else 0\n\n        harmonized_rows.append({\n            'image_path': image_path,\n            'dataset': 'RFMiD_v1',\n            'split': split,\n            'ID': image_id,\n            'N': N, 'D': D, 'G': G, 'C': C, 'A': A, 'H': H, 'M': M, 'O': O\n        })\n\n    return pd.DataFrame(harmonized_rows)\n\n\ndef harmonize_rfmid_v2(split: str) -> pd.DataFrame:\n    base_path = \"/kaggle/input/rdc-version-2/RFDiM2_0\"\n\n    if split == \"train\":\n        img_dir = f\"{base_path}/Training_set_2/Train_2\"\n        label_file = f\"{base_path}/Training_set_2/RFMiD_2_Training_labels.csv\"\n    elif split == \"val\":\n        img_dir = f\"{base_path}/Validation_set_2/Validation_2\"\n        label_file = f\"{base_path}/Validation_set_2/RFMiD_2_Validation_labels.csv\"\n    else:\n        img_dir = f\"{base_path}/Test_set_2/Test_2\"\n        label_file = f\"{base_path}/Test_set_2/RFMiD_2_Testing_labels.csv\"\n\n    try:\n        df = pd.read_csv(label_file, encoding='utf-8')\n    except UnicodeDecodeError:\n        df = pd.read_csv(label_file, encoding='latin1')\n\n    df.columns = df.columns.str.strip()\n\n    potential_disease_cols = ['AH', 'AION', 'ARMD', 'BRVO', 'CB', 'CF', 'CL', 'CME',\n                              'CNV', 'CRAO', 'CRS', 'CRVO', 'CSR', 'CWS', 'CSC', 'DN',\n                              'DR', 'EDN', 'ERM', 'GRT', 'HPED', 'HR', 'LS', 'MCA',\n                              'ME', 'MH', 'MHL', 'MS', 'MYA', 'ODC', 'ODE', 'ODP',\n                              'ON', 'OPDM', 'PRH', 'RD', 'RHL', 'RTR', 'RP', 'RPEC',\n                              'RS', 'RT', 'SOFE', 'ST', 'TD', 'TSLN', 'TV', 'VS',\n                              'HTN', 'IIH']\n\n    all_disease_cols = [col for col in potential_disease_cols if col in df.columns]\n    harmonized_rows = []\n\n    skipped_count = 0\n    found_count = 0\n\n    for _, row in df.iterrows():\n        image_id = int(row['ID'])\n\n        image_path = None\n        for ext in ['.jpg', '.JPG', '.png', '.PNG', '.jpeg', '.JPEG']:\n            candidate = os.path.join(img_dir, f\"{image_id}{ext}\")\n            if os.path.exists(candidate):\n                image_path = candidate\n                found_count += 1\n                break\n\n        if image_path is None:\n            skipped_count += 1\n            continue\n\n        wnl = row.get('WNL', 0)\n        N = 1 if wnl == 1 else 0\n        D = 1 if row.get('DR', 0) == 1 else 0\n        G = 1 if row.get('ODC', 0) == 1 else 0\n        C = 1 if row.get('MH', 0) == 1 else 0\n        A = 1 if row.get('ARMD', 0) == 1 else 0\n\n        # HTN is your \"H\" mapping; fallback to HR if HTN absent\n        H = 1 if row.get('HTN', 0) == 1 else 0\n        if H == 0 and 'HR' in df.columns:\n            H = 1 if row.get('HR', 0) == 1 else 0\n\n        M = 1 if row.get('MYA', 0) == 1 else 0\n\n        used_for_mapping = ['DR', 'ODC', 'MH', 'ARMD', 'HTN', 'HR', 'MYA', 'WNL']\n        other_cols = [col for col in all_disease_cols if col not in used_for_mapping]\n        O = 1 if any(row.get(col, 0) == 1 for col in other_cols) else 0\n\n        harmonized_rows.append({\n            'image_path': image_path,\n            'dataset': 'RFMiD_v2',\n            'split': split,\n            'ID': image_id,\n            'N': N, 'D': D, 'G': G, 'C': C, 'A': A, 'H': H, 'M': M, 'O': O\n        })\n\n    if skipped_count > 0:\n        print(f\"RFMiD_v2 {split}: Found {found_count}, Skipped {skipped_count}\")\n\n    return pd.DataFrame(harmonized_rows)\n\n\ndef harmonize_all_datasets() -> Dict[str, pd.DataFrame]:\n    results = {}\n    print(\"\\n\" + \"=\" * 60)\n    print(\"HARMONIZING DATASETS\")\n    print(\"=\" * 60)\n\n    for split in ['train', 'val', 'test']:\n        print(f\"\\nProcessing {split} split...\")\n        results[f'ODIR_{split}'] = harmonize_odir(split)\n        results[f'RFMiD_v1_{split}'] = harmonize_rfmid_v1(split)\n        results[f'RFMiD_v2_{split}'] = harmonize_rfmid_v2(split)\n\n    return results\n\n\ndef save_harmonized_data(harmonized_data: Dict[str, pd.DataFrame], output_dir: str):\n    os.makedirs(output_dir, exist_ok=True)\n\n    print(\"\\n\" + \"=\" * 60)\n    print(\"SAVING HARMONIZED DATA\")\n    print(\"=\" * 60)\n\n    for key, df in harmonized_data.items():\n        output_path = os.path.join(output_dir, f\"{key}.csv\")\n        df.to_csv(output_path, index=False)\n        print(f\"âœ… Saved {key:20s}: {len(df):5d} rows â†’ {output_path}\")\n\n    print(\"\\nðŸ“¦ Creating combined files...\")\n    for split in ['train', 'val', 'test']:\n        split_dfs = [v for k, v in harmonized_data.items() if k.endswith(f'_{split}')]\n        if split_dfs:\n            combined = pd.concat(split_dfs, ignore_index=True)\n            output_path = os.path.join(output_dir, f\"combined_{split}.csv\")\n            combined.to_csv(output_path, index=False)\n            print(f\"âœ… Saved combined_{split:5s}: {len(combined):5d} rows â†’ {output_path}\")\n\n    print(f\"\\nâœ¨ All files saved to: {output_dir}\")\n\n\ndef verify_images(harmonized_data: Dict[str, pd.DataFrame]):\n    print(\"\\n\" + \"=\" * 60)\n    print(\"VERIFYING IMAGE PATHS\")\n    print(\"=\" * 60)\n\n    all_good = True\n    for key, df in sorted(harmonized_data.items()):\n        missing = df[~df['image_path'].apply(os.path.exists)]\n        if len(missing) > 0:\n            print(f\"âŒ {key:20s}: {len(missing)} missing images\")\n            all_good = False\n        else:\n            print(f\"âœ… {key:20s}: All {len(df)} images found\")\n\n    if all_good:\n        print(\"\\nðŸŽ‰ All image paths verified successfully!\")\n    else:\n        print(\"\\nâš ï¸  Some images are missing (training will drop missing paths anyway).\")\n\n\ndef ensure_harmonized_csvs(output_dir: str, required_csvs: list[str]):\n    missing = [p for p in required_csvs if not os.path.exists(p)]\n    if not missing:\n        print(f\"[INFO] Harmonized CSVs already exist in: {output_dir}\")\n        return\n\n    print(\"\\n[INFO] Missing harmonized CSVs. Generating them now...\")\n    for p in missing[:10]:\n        print(f\"  - missing: {p}\")\n    if len(missing) > 10:\n        print(f\"  ... (+{len(missing) - 10} more)\")\n\n    harmonized_data = harmonize_all_datasets()\n    verify_images(harmonized_data)\n    save_harmonized_data(harmonized_data, output_dir=output_dir)\n\n\n# ============================================================\n# 2) TRAINING PIPELINE (Mixup DG) + Swin-T\n# ============================================================\n\n# =============================================================================\n# Config for FOLD (Swapped: Train=ODIR+RFMiD_v2, Test=RFMiD_v1)\n# =============================================================================\n\nSEED = 42\nSAVE_DIR = '/kaggle/working/results_lodo_mixup/fold_test_RFMiD_v1_swin_t'\nTEST_DOMAIN = \"RFMiD_v1\"\nTRAIN_DOMAINS = \"ODIR + RFMiD_v2\"\n\nMIXUP_ALPHA = 0.2\nOUTPUT_LABEL_DIR = \"/kaggle/working/harmonized_labels\"\n\n# Build CSV paths from OUTPUT_LABEL_DIR (so they match where we save)\nTEST_CSV = os.path.join(OUTPUT_LABEL_DIR, \"RFMiD_v1_test.csv\")\nTRAIN_CSVS = [\n    os.path.join(OUTPUT_LABEL_DIR, \"ODIR_train.csv\"),\n    os.path.join(OUTPUT_LABEL_DIR, \"RFMiD_v2_train.csv\"),\n]\nVAL_CSVS = [\n    os.path.join(OUTPUT_LABEL_DIR, \"ODIR_val.csv\"),\n    os.path.join(OUTPUT_LABEL_DIR, \"RFMiD_v2_val.csv\"),\n]\n\n\n# =============================================================================\n# Reproducibility\n# =============================================================================\n\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n\n\n# =============================================================================\n# Dataset (with domain tracking for Mixup!)\n# =============================================================================\n\nclass RetinalDataset(Dataset):\n    def __init__(self, csv_path, domain_id, transform=None):\n        self.data = pd.read_csv(csv_path)\n        self.domain_id = domain_id\n        self.transform = transform\n        self.label_cols = [\"N\", \"D\", \"G\", \"C\", \"A\", \"H\", \"M\", \"O\"]\n\n        dup = self.data.duplicated(subset=[\"image_path\"], keep=\"first\").sum()\n        if dup > 0:\n            print(f\"[WARN] {dup} duplicates found in {os.path.basename(csv_path)}, keeping first\")\n        self.data = self.data.drop_duplicates(subset=[\"image_path\"]).reset_index(drop=True)\n\n        valid_mask = self.data[\"image_path\"].apply(os.path.exists)\n        missing = (~valid_mask).sum()\n        if missing > 0:\n            print(f\"[WARN] {os.path.basename(csv_path)}: dropping {missing} missing images\")\n        self.data = self.data.loc[valid_mask].reset_index(drop=True)\n\n        print(f\"[INFO] Domain {domain_id}: Loaded {len(self.data)} valid images from {os.path.basename(csv_path)}\")\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        row = self.data.iloc[idx]\n        try:\n            image = Image.open(row[\"image_path\"]).convert(\"RGB\")\n        except:\n            image = Image.new(\"RGB\", (224, 224), color=(128, 128, 128))\n\n        labels = torch.tensor(row[self.label_cols].values.astype(\"float32\"))\n        if self.transform:\n            image = self.transform(image)\n        return image, labels, self.domain_id\n\n\n# =============================================================================\n# Pos weight\n# =============================================================================\n\ndef calculate_pos_weights(datasets, clip_min=0.5, clip_max=50.0):\n    all_labels = []\n    for ds in datasets:\n        all_labels.append(ds.data[[\"N\", \"D\", \"G\", \"C\", \"A\", \"H\", \"M\", \"O\"]].values)\n    combined = np.vstack(all_labels)\n\n    pos = combined.sum(axis=0)\n    neg = len(combined) - pos\n    raw = neg / (pos + 1e-5)\n    clipped = np.clip(raw, clip_min, clip_max)\n\n    print(\"\\n[INFO] Positive class weights (from training domains):\")\n    for i, col in enumerate([\"N\", \"D\", \"G\", \"C\", \"A\", \"H\", \"M\", \"O\"]):\n        print(f\"  {col}: {clipped[i]:.2f}\")\n\n    return torch.tensor(clipped, dtype=torch.float32)\n\n\n# =============================================================================\n# Model (Swin Transformer Tiny) with version-safe loading\n# =============================================================================\n\nclass SwinTMixupMultiLabel(nn.Module):\n    def __init__(self, num_classes=8, dropout=0.3, pretrained=True):\n        super().__init__()\n\n        # Version-safe Swin-T loading (torchvision may differ across Kaggle images)\n        if pretrained:\n            try:\n                weights = models.Swin_T_Weights.IMAGENET1K_V1\n                self.backbone = models.swin_t(weights=weights)\n            except Exception:\n                self.backbone = models.swin_t(pretrained=True)\n        else:\n            try:\n                self.backbone = models.swin_t(weights=None)\n            except Exception:\n                self.backbone = models.swin_t(pretrained=False)\n\n        # Remove original head and replace\n        in_features = self.backbone.head.in_features\n        self.backbone.head = nn.Identity()\n\n        self.classifier = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Linear(in_features, 512),\n            nn.ReLU(),\n            nn.BatchNorm1d(512),\n            nn.Dropout(dropout),\n            nn.Linear(512, num_classes),\n        )\n\n    def forward(self, x):\n        feats = self.backbone(x)\n        return self.classifier(feats)\n\n\n# =============================================================================\n# Transforms\n# =============================================================================\n\ndef get_transforms(is_train=False):\n    if is_train:\n        return transforms.Compose([\n            transforms.Resize((256, 256)),\n            transforms.RandomCrop(224),\n            transforms.RandomRotation(10),\n            transforms.ColorJitter(brightness=0.2, contrast=0.2),\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n        ])\n    else:\n        return transforms.Compose([\n            transforms.Resize((224, 224)),\n            transforms.ToTensor(),\n            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n        ])\n\n\n# =============================================================================\n# Metrics\n# =============================================================================\n\ndef compute_metrics(labels, probs, thresholds=None):\n    n_classes = labels.shape[1]\n\n    aucs = []\n    for i in range(n_classes):\n        if len(np.unique(labels[:, i])) > 1:\n            aucs.append(roc_auc_score(labels[:, i], probs[:, i]))\n        else:\n            aucs.append(np.nan)\n\n    aps = []\n    for i in range(n_classes):\n        if len(np.unique(labels[:, i])) > 1:\n            aps.append(average_precision_score(labels[:, i], probs[:, i]))\n        else:\n            aps.append(np.nan)\n\n    if thresholds is None:\n        thresholds = np.full(n_classes, 0.5)\n\n    preds = (probs >= thresholds).astype(int)\n    f1 = f1_score(labels, preds, average=\"macro\", zero_division=0)\n\n    return {\n        \"mAUC\": float(np.nanmean(aucs)),\n        \"mAP\": float(np.nanmean(aps)),\n        \"per_class_auc\": aucs,\n        \"per_class_ap\": aps,\n        \"macro_f1\": float(f1),\n    }\n\n\ndef find_optimal_thresholds(labels, probs):\n    n_classes = labels.shape[1]\n    thresholds = []\n    search_range = np.linspace(0.05, 0.95, 91)\n\n    for i in range(n_classes):\n        best_f1, best_t = 0.0, 0.5\n        if len(np.unique(labels[:, i])) > 1:\n            for t in search_range:\n                preds = (probs[:, i] >= t).astype(int)\n                f1 = f1_score(labels[:, i], preds, zero_division=0)\n                if f1 > best_f1:\n                    best_f1, best_t = f1, t\n        thresholds.append(best_t)\n\n    return np.array(thresholds)\n\n\n# =============================================================================\n# MIXUP TRAINING\n# =============================================================================\n\ndef train_epoch_mixup(model, loaders_dict, criterion, optimizer, device, mixup_alpha=0.2):\n    model.train()\n    losses = []\n    mixup_stats = {'total_batches': 0, 'mixup_batches': 0}\n\n    domain_iters = {k: iter(v) for k, v in loaders_dict.items()}\n    domain_ids = list(loaders_dict.keys())\n    max_batches = max(len(loader) for loader in loaders_dict.values())\n\n    pbar = tqdm(range(max_batches), desc=\"Train (Mixup)\", leave=False)\n\n    for _ in pbar:\n        if len(domain_ids) >= 2 and np.random.rand() > 0.5:\n            d1, d2 = np.random.choice(domain_ids, size=2, replace=False)\n\n            try:\n                imgs1, labels1, _ = next(domain_iters[d1])\n            except StopIteration:\n                domain_iters[d1] = iter(loaders_dict[d1])\n                imgs1, labels1, _ = next(domain_iters[d1])\n\n            try:\n                imgs2, labels2, _ = next(domain_iters[d2])\n            except StopIteration:\n                domain_iters[d2] = iter(loaders_dict[d2])\n                imgs2, labels2, _ = next(domain_iters[d2])\n\n            min_size = min(imgs1.size(0), imgs2.size(0))\n            imgs1, labels1 = imgs1[:min_size], labels1[:min_size]\n            imgs2, labels2 = imgs2[:min_size], labels2[:min_size]\n\n            lam = np.random.beta(mixup_alpha, mixup_alpha)\n\n            mixed_imgs = lam * imgs1 + (1 - lam) * imgs2\n            mixed_labels = lam * labels1 + (1 - lam) * labels2\n\n            mixed_imgs = mixed_imgs.to(device)\n            mixed_labels = mixed_labels.to(device)\n\n            optimizer.zero_grad()\n            logits = model(mixed_imgs)\n            loss = criterion(logits, mixed_labels)\n\n            mixup_stats['mixup_batches'] += 1\n        else:\n            d = np.random.choice(domain_ids)\n            try:\n                imgs, labels, _ = next(domain_iters[d])\n            except StopIteration:\n                domain_iters[d] = iter(loaders_dict[d])\n                imgs, labels, _ = next(domain_iters[d])\n\n            imgs, labels = imgs.to(device), labels.to(device)\n\n            optimizer.zero_grad()\n            logits = model(imgs)\n            loss = criterion(logits, labels)\n\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n\n        losses.append(loss.item())\n        mixup_stats['total_batches'] += 1\n        pbar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n\n    return {\n        \"loss\": float(np.mean(losses)),\n        \"mixup_ratio\": mixup_stats['mixup_batches'] / max(1, mixup_stats['total_batches'])\n    }\n\n\n@torch.no_grad()\ndef validate(model, loader, criterion, device, thresholds=None):\n    model.eval()\n    losses, all_probs, all_labels = [], [], []\n\n    for batch in tqdm(loader, desc=\"Val\", leave=False):\n        if len(batch) == 3:\n            images, labels, _ = batch\n        else:\n            images, labels = batch\n\n        images, labels = images.to(device), labels.to(device)\n        logits = model(images)\n        loss = criterion(logits, labels)\n\n        losses.append(loss.item())\n        all_probs.append(torch.sigmoid(logits).cpu().numpy())\n        all_labels.append(labels.cpu().numpy())\n\n    probs = np.vstack(all_probs)\n    labels = np.vstack(all_labels)\n    metrics = compute_metrics(labels, probs, thresholds)\n    metrics[\"loss\"] = float(np.mean(losses))\n    return metrics, probs, labels\n\n\n# =============================================================================\n# Visualizations\n# =============================================================================\n\ndef plot_training_curves(history, save_dir):\n    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n    epochs = range(1, len(history['train_loss']) + 1)\n\n    axes[0].plot(epochs, history['train_loss'], marker='o', linewidth=2, label='Train Loss')\n    axes[0].plot(epochs, history['val_loss'], marker='s', linewidth=2, label='Val Loss')\n    axes[0].set_xlabel('Epoch')\n    axes[0].set_ylabel('Loss')\n    axes[0].set_title('Training & Validation Loss (Mixup, Swin-T)')\n    axes[0].legend()\n    axes[0].grid(True, alpha=0.3)\n\n    axes[1].plot(epochs, history['val_auc'], marker='s', linewidth=2, label='Val mAUC')\n    axes[1].set_xlabel('Epoch')\n    axes[1].set_ylabel('mAUC')\n    axes[1].set_title('Validation mAUC (Mixup, Swin-T)')\n    axes[1].legend()\n    axes[1].grid(True, alpha=0.3)\n    axes[1].set_ylim([0, 1.0])\n\n    os.makedirs(save_dir, exist_ok=True)\n    plt.tight_layout()\n    plt.savefig(f'{save_dir}/training_curves.png', dpi=300, bbox_inches='tight')\n    plt.close()\n    print(f\"[INFO] âœ“ Saved training curves\")\n\n\ndef plot_per_class_roc(labels, probs, test_domain, save_dir):\n    class_names = [\"N\", \"D\", \"G\", \"C\", \"A\", \"H\", \"M\", \"O\"]\n    fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n    axes = axes.flatten()\n\n    for i in range(8):\n        if len(np.unique(labels[:, i])) > 1:\n            fpr, tpr, _ = roc_curve(labels[:, i], probs[:, i])\n            auc = roc_auc_score(labels[:, i], probs[:, i])\n            axes[i].plot(fpr, tpr, linewidth=2, label=f'AUC = {auc:.3f}')\n            axes[i].plot([0, 1], [0, 1], 'k--', linewidth=1, alpha=0.5)\n            axes[i].set_title(f'Class {class_names[i]}')\n            axes[i].set_xlabel('FPR')\n            axes[i].set_ylabel('TPR')\n            axes[i].legend(loc='lower right')\n            axes[i].grid(True, alpha=0.3)\n        else:\n            axes[i].text(0.5, 0.5, 'Single class\\n(No ROC)', ha='center', va='center')\n            axes[i].set_title(f'Class {class_names[i]}')\n\n    plt.suptitle(f'Per-Class ROC Curves - Test on {test_domain} (Mixup, Swin-T)', y=0.995)\n    os.makedirs(save_dir, exist_ok=True)\n    plt.tight_layout()\n    plt.savefig(f'{save_dir}/per_class_roc_curves.png', dpi=300, bbox_inches='tight')\n    plt.close()\n    print(f\"[INFO] âœ“ Saved per-class ROC curves\")\n\n\ndef plot_macro_roc(labels, probs, test_domain, save_dir):\n    all_fpr, all_tpr = [], []\n    for i in range(8):\n        if len(np.unique(labels[:, i])) > 1:\n            fpr, tpr, _ = roc_curve(labels[:, i], probs[:, i])\n            all_fpr.append(fpr)\n            all_tpr.append(tpr)\n\n    mean_fpr = np.linspace(0, 1, 100)\n    tprs = [np.interp(mean_fpr, fpr, tpr) for fpr, tpr in zip(all_fpr, all_tpr)]\n    mean_tpr = np.mean(tprs, axis=0)\n    macro_auc = np.trapz(mean_tpr, mean_fpr)\n\n    plt.figure(figsize=(8, 6))\n    plt.plot(mean_fpr, mean_tpr, linewidth=3, label=f'Macro ROC (AUC = {macro_auc:.3f})')\n    plt.plot([0, 1], [0, 1], 'k--', linewidth=2, alpha=0.5, label='Random')\n    plt.xlabel('FPR')\n    plt.ylabel('TPR')\n    plt.title(f'Macro ROC - Test on {test_domain} (Mixup, Swin-T)')\n    plt.legend(loc='lower right')\n    plt.grid(True, alpha=0.3)\n\n    os.makedirs(save_dir, exist_ok=True)\n    plt.tight_layout()\n    plt.savefig(f'{save_dir}/macro_roc_curve.png', dpi=300, bbox_inches='tight')\n    plt.close()\n    print(f\"[INFO] âœ“ Saved macro ROC curve\")\n\n\ndef plot_per_class_metrics(test_metrics, save_dir):\n    class_names = [\"N\", \"D\", \"G\", \"C\", \"A\", \"H\", \"M\", \"O\"]\n    aucs = [0 if np.isnan(x) else x for x in test_metrics['per_class_auc']]\n    aps = [0 if np.isnan(x) else x for x in test_metrics['per_class_ap']]\n\n    x = np.arange(len(class_names))\n    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n    axes[0].bar(x, aucs, alpha=0.8, edgecolor='black', linewidth=1.2)\n    axes[0].set_xticks(x)\n    axes[0].set_xticklabels(class_names)\n    axes[0].set_ylim([0, 1.0])\n    axes[0].set_title('Per-Class AUC (Mixup, Swin-T)')\n    axes[0].grid(True, alpha=0.3, axis='y')\n\n    axes[1].bar(x, aps, alpha=0.8, edgecolor='black', linewidth=1.2)\n    axes[1].set_xticks(x)\n    axes[1].set_xticklabels(class_names)\n    axes[1].set_ylim([0, 1.0])\n    axes[1].set_title('Per-Class AP (Mixup, Swin-T)')\n    axes[1].grid(True, alpha=0.3, axis='y')\n\n    os.makedirs(save_dir, exist_ok=True)\n    plt.tight_layout()\n    plt.savefig(f'{save_dir}/per_class_metrics.png', dpi=300, bbox_inches='tight')\n    plt.close()\n    print(f\"[INFO] âœ“ Saved per-class metrics\")\n\n\n# =============================================================================\n# MAIN\n# =============================================================================\n\nif __name__ == \"__main__\":\n    set_seed(SEED)\n    os.makedirs(SAVE_DIR, exist_ok=True)\n\n    # FIX: ensure the required harmonized CSVs exist BEFORE reading them\n    required = TRAIN_CSVS + VAL_CSVS + [TEST_CSV]\n    ensure_harmonized_csvs(output_dir=OUTPUT_LABEL_DIR, required_csvs=required)\n\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n    print(\"=\" * 80)\n    print(f\"LODO FOLD (SWAPPED) with MIXUP DG: Test on {TEST_DOMAIN}\")\n    print(f\"Training on: {TRAIN_DOMAINS}\")\n    print(\"=\" * 80)\n    print(f\"Device: {device}\")\n    print(f\"Seed: {SEED}\")\n    print(f\"Mixup Alpha: {MIXUP_ALPHA}\")\n    print(\"=\" * 80)\n\n    print(\"\\n[INFO] Loading datasets...\")\n    train_datasets = [RetinalDataset(csv, domain_id=i, transform=get_transforms(True))\n                      for i, csv in enumerate(TRAIN_CSVS)]\n    val_datasets = [RetinalDataset(csv, domain_id=i, transform=get_transforms(False))\n                    for i, csv in enumerate(VAL_CSVS)]\n    test_dataset = RetinalDataset(TEST_CSV, domain_id=999, transform=get_transforms(False))\n\n    print(f\"\\n[INFO] Total train: {sum(len(ds) for ds in train_datasets)} images\")\n    print(f\"[INFO] Total val: {sum(len(ds) for ds in val_datasets)} images\")\n    print(f\"[INFO] Test: {len(test_dataset)} images\")\n\n    pos_weights = calculate_pos_weights(train_datasets).to(device)\n\n    # DataLoaders (separate per domain for mixup)\n    g = torch.Generator().manual_seed(SEED)\n    train_loaders = {\n        i: DataLoader(ds, batch_size=32, shuffle=True, num_workers=4,\n                      pin_memory=True, generator=g)\n        for i, ds in enumerate(train_datasets)\n    }\n\n    combined_val = ConcatDataset(val_datasets)\n    val_loader = DataLoader(combined_val, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)\n    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=4, pin_memory=True)\n\n    print(\"\\n[INFO] Initializing Swin-T model...\")\n    model = SwinTMixupMultiLabel(num_classes=8, dropout=0.3, pretrained=True).to(device)\n\n    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weights)\n\n    # AdamW is common for transformers\n    optimizer = optim.AdamW([\n        {'params': model.backbone.parameters(), 'lr': 5e-5},\n        {'params': model.classifier.parameters(), 'lr': 5e-4}\n    ], weight_decay=1e-4)\n\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3)\n\n    history = {'train_loss': [], 'val_loss': [], 'val_auc': []}\n    best_val_auc, patience_counter = 0.0, 0\n\n    print(\"\\n[INFO] Training started with Mixup DG (Swin-T)...\")\n    print(\"-\" * 80)\n\n    for epoch in range(50):\n        train_metrics = train_epoch_mixup(model, train_loaders, criterion, optimizer, device, mixup_alpha=MIXUP_ALPHA)\n        val_metrics, _, _ = validate(model, val_loader, criterion, device)\n        scheduler.step(val_metrics['mAUC'])\n\n        history['train_loss'].append(train_metrics['loss'])\n        history['val_loss'].append(val_metrics['loss'])\n        history['val_auc'].append(val_metrics['mAUC'])\n\n        print(f\"Epoch {epoch+1:02d} | Train Loss: {train_metrics['loss']:.4f} \"\n              f\"| Val mAUC: {val_metrics['mAUC']:.4f} | Mixup: {train_metrics['mixup_ratio']:.1%}\")\n\n        if val_metrics['mAUC'] > best_val_auc:\n            best_val_auc = val_metrics['mAUC']\n            torch.save(model.state_dict(), f'{SAVE_DIR}/best_model.pth')\n            print(f\"  âœ“ Saved best model (val mAUC: {best_val_auc:.4f})\")\n            patience_counter = 0\n        else:\n            patience_counter += 1\n\n        if patience_counter >= 10:\n            print(f\"\\n[INFO] Early stopping at epoch {epoch+1}\")\n            break\n\n    print(f\"\\n[INFO] Loading best model...\")\n    model.load_state_dict(torch.load(f'{SAVE_DIR}/best_model.pth', map_location=device))\n    print(f\"[INFO] Best validation mAUC: {best_val_auc:.4f}\")\n\n    print(f\"\\n[INFO] Finding optimal thresholds on validation...\")\n    _, val_probs, val_labels = validate(model, val_loader, criterion, device)\n    thresholds = find_optimal_thresholds(val_labels, val_probs)\n\n    class_names = [\"N\", \"D\", \"G\", \"C\", \"A\", \"H\", \"M\", \"O\"]\n    print(\"[INFO] Optimal thresholds:\")\n    for c, t in zip(class_names, thresholds):\n        print(f\"  {c}: {t:.3f}\")\n\n    print(f\"\\n[INFO] Testing on {TEST_DOMAIN}...\")\n    print(\"-\" * 80)\n    test_metrics, test_probs, test_labels = validate(model, test_loader, criterion, device, thresholds)\n\n    print(\"\\n\" + \"=\" * 80)\n    print(f\"TEST RESULTS - {TEST_DOMAIN} (Mixup DG, Swin-T)\")\n    print(\"=\" * 80)\n    print(f\"mAUC:      {test_metrics['mAUC']:.4f}\")\n    print(f\"mAP:       {test_metrics['mAP']:.4f}\")\n    print(f\"Macro F1:  {test_metrics['macro_f1']:.4f}\")\n    print(\"=\" * 80)\n\n    print(f\"\\n{'Class':<8} {'AUC':<10} {'AP':<10}\")\n    print(\"-\" * 30)\n    for i, cls in enumerate(class_names):\n        auc = test_metrics['per_class_auc'][i]\n        ap = test_metrics['per_class_ap'][i]\n        auc_str = f\"{auc:.4f}\" if not np.isnan(auc) else \"N/A\"\n        ap_str = f\"{ap:.4f}\" if not np.isnan(ap) else \"N/A\"\n        print(f\"{cls:<8} {auc_str:<10} {ap_str:<10}\")\n    print(\"-\" * 30)\n\n    print(\"\\n[INFO] Saving results...\")\n    results_df = pd.DataFrame([{\n        'method': 'Mixup',\n        'backbone': 'swin_t',\n        'test_domain': TEST_DOMAIN,\n        'train_domains': TRAIN_DOMAINS,\n        'mAUC': test_metrics['mAUC'],\n        'mAP': test_metrics['mAP'],\n        'macro_f1': test_metrics['macro_f1'],\n        'best_val_auc': best_val_auc,\n        'mixup_alpha': MIXUP_ALPHA\n    }])\n    results_df.to_csv(f'{SAVE_DIR}/test_results.csv', index=False)\n    print(f\"[INFO] âœ“ Saved test_results.csv\")\n\n    print(\"\\n[INFO] Generating visualizations...\")\n    plot_training_curves(history, SAVE_DIR)\n    plot_per_class_roc(test_labels, test_probs, TEST_DOMAIN, SAVE_DIR)\n    plot_macro_roc(test_labels, test_probs, TEST_DOMAIN, SAVE_DIR)\n    plot_per_class_metrics(test_metrics, SAVE_DIR)\n\n    print(\"\\n\" + \"=\" * 80)\n    print(\"âœ“ MIXUP LODO FOLD (SWAPPED) COMPLETE!\")\n    print(f\"âœ“ Results saved to: {SAVE_DIR}/\")\n    print(\"=\" * 80)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-18T03:26:10.016750Z","iopub.execute_input":"2026-01-18T03:26:10.017303Z","iopub.status.idle":"2026-01-18T04:48:53.269892Z","shell.execute_reply.started":"2026-01-18T03:26:10.017271Z","shell.execute_reply":"2026-01-18T04:48:53.268995Z"}},"outputs":[{"name":"stdout","text":"\n[INFO] Missing harmonized CSVs. Generating them now...\n  - missing: /kaggle/working/harmonized_labels/ODIR_train.csv\n  - missing: /kaggle/working/harmonized_labels/RFMiD_v2_train.csv\n  - missing: /kaggle/working/harmonized_labels/ODIR_val.csv\n  - missing: /kaggle/working/harmonized_labels/RFMiD_v2_val.csv\n  - missing: /kaggle/working/harmonized_labels/RFMiD_v1_test.csv\n\n============================================================\nHARMONIZING DATASETS\n============================================================\n\nProcessing train split...\nRFMiD_v2 train: Found 507, Skipped 2\n\nProcessing val split...\n\nProcessing test split...\nRFMiD_v2 test: Found 170, Skipped 4\n\n============================================================\nVERIFYING IMAGE PATHS\n============================================================\nâœ… ODIR_test           : All 2000 images found\nâœ… ODIR_train          : All 7000 images found\nâœ… ODIR_val            : All 1000 images found\nâœ… RFMiD_v1_test       : All 640 images found\nâœ… RFMiD_v1_train      : All 1920 images found\nâœ… RFMiD_v1_val        : All 640 images found\nâœ… RFMiD_v2_test       : All 170 images found\nâœ… RFMiD_v2_train      : All 507 images found\nâœ… RFMiD_v2_val        : All 177 images found\n\nðŸŽ‰ All image paths verified successfully!\n\n============================================================\nSAVING HARMONIZED DATA\n============================================================\nâœ… Saved ODIR_train          :  7000 rows â†’ /kaggle/working/harmonized_labels/ODIR_train.csv\nâœ… Saved RFMiD_v1_train      :  1920 rows â†’ /kaggle/working/harmonized_labels/RFMiD_v1_train.csv\nâœ… Saved RFMiD_v2_train      :   507 rows â†’ /kaggle/working/harmonized_labels/RFMiD_v2_train.csv\nâœ… Saved ODIR_val            :  1000 rows â†’ /kaggle/working/harmonized_labels/ODIR_val.csv\nâœ… Saved RFMiD_v1_val        :   640 rows â†’ /kaggle/working/harmonized_labels/RFMiD_v1_val.csv\nâœ… Saved RFMiD_v2_val        :   177 rows â†’ /kaggle/working/harmonized_labels/RFMiD_v2_val.csv\nâœ… Saved ODIR_test           :  2000 rows â†’ /kaggle/working/harmonized_labels/ODIR_test.csv\nâœ… Saved RFMiD_v1_test       :   640 rows â†’ /kaggle/working/harmonized_labels/RFMiD_v1_test.csv\nâœ… Saved RFMiD_v2_test       :   170 rows â†’ /kaggle/working/harmonized_labels/RFMiD_v2_test.csv\n\nðŸ“¦ Creating combined files...\nâœ… Saved combined_train:  9427 rows â†’ /kaggle/working/harmonized_labels/combined_train.csv\nâœ… Saved combined_val  :  1817 rows â†’ /kaggle/working/harmonized_labels/combined_val.csv\nâœ… Saved combined_test :  2810 rows â†’ /kaggle/working/harmonized_labels/combined_test.csv\n\nâœ¨ All files saved to: /kaggle/working/harmonized_labels\n================================================================================\nLODO FOLD (SWAPPED) with MIXUP DG: Test on RFMiD_v1\nTraining on: ODIR + RFMiD_v2\n================================================================================\nDevice: cuda\nSeed: 42\nMixup Alpha: 0.2\n================================================================================\n\n[INFO] Loading datasets...\n[INFO] Domain 0: Loaded 7000 valid images from ODIR_train.csv\n[INFO] Domain 1: Loaded 507 valid images from RFMiD_v2_train.csv\n[INFO] Domain 0: Loaded 1000 valid images from ODIR_val.csv\n[INFO] Domain 1: Loaded 177 valid images from RFMiD_v2_val.csv\n[INFO] Domain 999: Loaded 640 valid images from RFMiD_v1_test.csv\n\n[INFO] Total train: 7507 images\n[INFO] Total val: 1177 images\n[INFO] Test: 640 images\n\n[INFO] Positive class weights (from training domains):\n  N: 2.08\n  D: 2.27\n  G: 15.65\n  C: 15.68\n  A: 21.54\n  H: 26.80\n  M: 19.02\n  O: 2.39\n\n[INFO] Initializing Swin-T model...\nDownloading: \"https://download.pytorch.org/models/swin_t-704ceda3.pth\" to /root/.cache/torch/hub/checkpoints/swin_t-704ceda3.pth\n","output_type":"stream"},{"name":"stderr","text":"100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 108M/108M [00:00<00:00, 164MB/s]  \n","output_type":"stream"},{"name":"stdout","text":"\n[INFO] Training started with Mixup DG (Swin-T)...\n--------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"                                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 01 | Train Loss: 0.9625 | Val mAUC: 0.7745 | Mixup: 53.9%\n  âœ“ Saved best model (val mAUC: 0.7745)\n","output_type":"stream"},{"name":"stderr","text":"                                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 02 | Train Loss: 0.7650 | Val mAUC: 0.8071 | Mixup: 44.3%\n  âœ“ Saved best model (val mAUC: 0.8071)\n","output_type":"stream"},{"name":"stderr","text":"                                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 03 | Train Loss: 0.6977 | Val mAUC: 0.8077 | Mixup: 51.6%\n  âœ“ Saved best model (val mAUC: 0.8077)\n","output_type":"stream"},{"name":"stderr","text":"                                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 04 | Train Loss: 0.6667 | Val mAUC: 0.8183 | Mixup: 48.4%\n  âœ“ Saved best model (val mAUC: 0.8183)\n","output_type":"stream"},{"name":"stderr","text":"                                                                             ","output_type":"stream"},{"name":"stdout","text":"Epoch 05 | Train Loss: 0.6472 | Val mAUC: 0.8064 | Mixup: 50.2%\n","output_type":"stream"},{"name":"stderr","text":"                                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 06 | Train Loss: 0.6490 | Val mAUC: 0.8248 | Mixup: 46.6%\n  âœ“ Saved best model (val mAUC: 0.8248)\n","output_type":"stream"},{"name":"stderr","text":"                                                                             ","output_type":"stream"},{"name":"stdout","text":"Epoch 07 | Train Loss: 0.5977 | Val mAUC: 0.8211 | Mixup: 53.4%\n","output_type":"stream"},{"name":"stderr","text":"                                                                             ","output_type":"stream"},{"name":"stdout","text":"Epoch 08 | Train Loss: 0.5688 | Val mAUC: 0.8142 | Mixup: 46.6%\n","output_type":"stream"},{"name":"stderr","text":"                                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 09 | Train Loss: 0.6385 | Val mAUC: 0.8202 | Mixup: 52.1%\n","output_type":"stream"},{"name":"stderr","text":"                                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 10 | Train Loss: 0.5353 | Val mAUC: 0.8276 | Mixup: 52.5%\n  âœ“ Saved best model (val mAUC: 0.8276)\n","output_type":"stream"},{"name":"stderr","text":"                                                                             ","output_type":"stream"},{"name":"stdout","text":"Epoch 11 | Train Loss: 0.5767 | Val mAUC: 0.8109 | Mixup: 48.4%\n","output_type":"stream"},{"name":"stderr","text":"                                                                             ","output_type":"stream"},{"name":"stdout","text":"Epoch 12 | Train Loss: 0.6155 | Val mAUC: 0.8178 | Mixup: 51.1%\n","output_type":"stream"},{"name":"stderr","text":"                                                                             ","output_type":"stream"},{"name":"stdout","text":"Epoch 13 | Train Loss: 0.5127 | Val mAUC: 0.8243 | Mixup: 47.5%\n","output_type":"stream"},{"name":"stderr","text":"                                                                             ","output_type":"stream"},{"name":"stdout","text":"Epoch 14 | Train Loss: 0.5370 | Val mAUC: 0.8150 | Mixup: 52.1%\n","output_type":"stream"},{"name":"stderr","text":"                                                                             ","output_type":"stream"},{"name":"stdout","text":"Epoch 15 | Train Loss: 0.5042 | Val mAUC: 0.8246 | Mixup: 42.5%\n","output_type":"stream"},{"name":"stderr","text":"                                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 16 | Train Loss: 0.5234 | Val mAUC: 0.8315 | Mixup: 46.6%\n  âœ“ Saved best model (val mAUC: 0.8315)\n","output_type":"stream"},{"name":"stderr","text":"                                                                             ","output_type":"stream"},{"name":"stdout","text":"Epoch 17 | Train Loss: 0.5033 | Val mAUC: 0.8224 | Mixup: 50.7%\n","output_type":"stream"},{"name":"stderr","text":"                                                                             ","output_type":"stream"},{"name":"stdout","text":"Epoch 18 | Train Loss: 0.4592 | Val mAUC: 0.8158 | Mixup: 46.1%\n","output_type":"stream"},{"name":"stderr","text":"                                                                             ","output_type":"stream"},{"name":"stdout","text":"Epoch 19 | Train Loss: 0.4968 | Val mAUC: 0.8221 | Mixup: 53.4%\n","output_type":"stream"},{"name":"stderr","text":"                                                                             ","output_type":"stream"},{"name":"stdout","text":"Epoch 20 | Train Loss: 0.4183 | Val mAUC: 0.8307 | Mixup: 48.4%\n","output_type":"stream"},{"name":"stderr","text":"                                                                             ","output_type":"stream"},{"name":"stdout","text":"Epoch 21 | Train Loss: 0.4419 | Val mAUC: 0.8280 | Mixup: 50.2%\n","output_type":"stream"},{"name":"stderr","text":"                                                                             ","output_type":"stream"},{"name":"stdout","text":"Epoch 22 | Train Loss: 0.4696 | Val mAUC: 0.8254 | Mixup: 54.3%\n","output_type":"stream"},{"name":"stderr","text":"                                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 23 | Train Loss: 0.4400 | Val mAUC: 0.8251 | Mixup: 54.3%\n","output_type":"stream"},{"name":"stderr","text":"                                                                             ","output_type":"stream"},{"name":"stdout","text":"Epoch 24 | Train Loss: 0.4383 | Val mAUC: 0.8309 | Mixup: 50.2%\n","output_type":"stream"},{"name":"stderr","text":"                                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 25 | Train Loss: 0.4456 | Val mAUC: 0.8293 | Mixup: 53.0%\n","output_type":"stream"},{"name":"stderr","text":"                                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 26 | Train Loss: 0.4310 | Val mAUC: 0.8294 | Mixup: 51.6%\n\n[INFO] Early stopping at epoch 26\n\n[INFO] Loading best model...\n[INFO] Best validation mAUC: 0.8315\n\n[INFO] Finding optimal thresholds on validation...\n","output_type":"stream"},{"name":"stderr","text":"                                                    \r","output_type":"stream"},{"name":"stdout","text":"[INFO] Optimal thresholds:\n  N: 0.290\n  D: 0.210\n  G: 0.750\n  C: 0.950\n  A: 0.910\n  H: 0.950\n  M: 0.950\n  O: 0.530\n\n[INFO] Testing on RFMiD_v1...\n--------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"                                                    \r","output_type":"stream"},{"name":"stdout","text":"\n================================================================================\nTEST RESULTS - RFMiD_v1 (Mixup DG, Swin-T)\n================================================================================\nmAUC:      0.8690\nmAP:       0.5245\nMacro F1:  0.4176\n================================================================================\n\nClass    AUC        AP        \n------------------------------\nN        0.9370     0.7462    \nD        0.9060     0.6821    \nG        0.7342     0.4316    \nC        0.9013     0.7460    \nA        0.9030     0.4377    \nH        0.9531     0.0323    \nM        0.9492     0.5532    \nO        0.6682     0.5671    \n------------------------------\n\n[INFO] Saving results...\n[INFO] âœ“ Saved test_results.csv\n\n[INFO] Generating visualizations...\n[INFO] âœ“ Saved training curves\n[INFO] âœ“ Saved per-class ROC curves\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_55/3624701511.py:690: DeprecationWarning: `trapz` is deprecated. Use `trapezoid` instead, or one of the numerical integration functions in `scipy.integrate`.\n  macro_auc = np.trapz(mean_tpr, mean_fpr)\n","output_type":"stream"},{"name":"stdout","text":"[INFO] âœ“ Saved macro ROC curve\n[INFO] âœ“ Saved per-class metrics\n\n================================================================================\nâœ“ MIXUP LODO FOLD (SWAPPED) COMPLETE!\nâœ“ Results saved to: /kaggle/working/results_lodo_mixup/fold_test_RFMiD_v1_swin_t/\n================================================================================\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}