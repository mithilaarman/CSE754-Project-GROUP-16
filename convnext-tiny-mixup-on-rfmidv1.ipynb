{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":2530487,"sourceType":"datasetVersion","datasetId":1533360},{"sourceId":13894314,"sourceType":"datasetVersion","datasetId":8852034},{"sourceId":13811294,"sourceType":"datasetVersion","datasetId":8794300}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ============================================================\n# FULL UPDATED CODE: ConvNeXt-Tiny + Mixup DG (FIXED SHAPES)\n# Fixes:\n#  - mean/std KeyError (no weights.meta[\"mean\"])\n#  - ImageClassification preset has no .transforms attribute\n#  - ConvNeXt output is [B, C, 1, 1] -> flatten to [B, C] before Linear\n# ============================================================\n\nimport os\nimport random\nfrom typing import Dict, Tuple, Any\n\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader, ConcatDataset\n\nimport torchvision.transforms as transforms\nimport torchvision.models as models\nfrom torchvision.models import ConvNeXt_Tiny_Weights\n\nfrom sklearn.metrics import roc_auc_score, f1_score, roc_curve, average_precision_score\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n\n\n# ============================================================\n# 1) HARMONIZATION\n# ============================================================\n\ndef harmonize_odir(split: str) -> pd.DataFrame:\n    base_path = \"/kaggle/input/odir-clr/ODIR_CLR\"\n\n    if split == \"train\":\n        img_dir = f\"{base_path}/Training_ Set/train_images\"\n        label_file = f\"{base_path}/Training_ Set/train_annotation.xlsx\"\n    elif split == \"val\":\n        img_dir = f\"{base_path}/Validation_set/val_images\"\n        label_file = f\"{base_path}/Validation_set/val_annotation.xlsx\"\n    else:\n        img_dir = f\"{base_path}/Test_Set/test_images\"\n        label_file = f\"{base_path}/Test_Set/test_annotation.xlsx\"\n\n    df = pd.read_excel(label_file)\n    label_cols = ['N', 'D', 'G', 'C', 'A', 'H', 'M', 'O']\n    harmonized_rows = []\n\n    for _, row in df.iterrows():\n        patient_id = row['ID']\n        labels = {col: int(row[col]) for col in label_cols}\n\n        if pd.notna(row['Left-Fundus']) and str(row['Left-Fundus']).strip():\n            left_path = os.path.join(img_dir, f\"{patient_id}_left.jpg\")\n            harmonized_rows.append({\n                'image_path': left_path,\n                'dataset': 'ODIR',\n                'split': split,\n                'ID': patient_id,\n                **labels\n            })\n\n        if pd.notna(row['Right-Fundus']) and str(row['Right-Fundus']).strip():\n            right_path = os.path.join(img_dir, f\"{patient_id}_right.jpg\")\n            harmonized_rows.append({\n                'image_path': right_path,\n                'dataset': 'ODIR',\n                'split': split,\n                'ID': patient_id,\n                **labels\n            })\n\n    return pd.DataFrame(harmonized_rows)\n\n\ndef harmonize_rfmid_v1(split: str) -> pd.DataFrame:\n    base_path = \"/kaggle/input/retinal-disease-classification\"\n\n    if split == \"train\":\n        img_dir = f\"{base_path}/Training_Set/Training_Set/Training\"\n        label_file = f\"{base_path}/Training_Set/Training_Set/RFMiD_Training_Labels.csv\"\n    elif split == \"val\":\n        img_dir = f\"{base_path}/Evaluation_Set/Evaluation_Set/Validation\"\n        label_file = f\"{base_path}/Evaluation_Set/Evaluation_Set/RFMiD_Validation_Labels.csv\"\n    else:\n        img_dir = f\"{base_path}/Test_Set/Test_Set/Test\"\n        label_file = f\"{base_path}/Test_Set/Test_Set/RFMiD_Testing_Labels.csv\"\n\n    df = pd.read_csv(label_file)\n\n    all_disease_cols = [\n        'DR', 'ARMD', 'MH', 'DN', 'MYA', 'BRVO', 'TSLN', 'ERM', 'LS', 'MS', 'CSR',\n        'ODC', 'CRVO', 'TV', 'AH', 'ODP', 'ODE', 'ST', 'AION', 'PT', 'RT', 'RS',\n        'CRS', 'EDN', 'RPEC', 'MHL', 'RP', 'CWS', 'CB', 'ODPM', 'PRH', 'MNF',\n        'HR', 'CRAO', 'TD', 'CME', 'PTCR', 'CF', 'VH', 'MCA', 'VS', 'BRAO',\n        'PLQ', 'HPED', 'CL'\n    ]\n\n    harmonized_rows = []\n\n    for _, row in df.iterrows():\n        image_id = row['ID']\n\n        image_path = None\n        for ext in ['.png', '.jpg', '.jpeg']:\n            candidate = os.path.join(img_dir, f\"{image_id}{ext}\")\n            if os.path.exists(candidate):\n                image_path = candidate\n                break\n        if image_path is None:\n            image_path = os.path.join(img_dir, str(image_id))\n\n        N = 1 if row['Disease_Risk'] == 0 else 0\n        D = 1 if row['DR'] == 1 else 0\n        G = 1 if row['ODC'] == 1 else 0\n        C = 1 if row['MH'] == 1 else 0\n        A = 1 if row['ARMD'] == 1 else 0\n        H = 1 if row['HR'] == 1 else 0\n        M = 1 if row['MYA'] == 1 else 0\n\n        used_for_mapping = ['DR', 'ODC', 'MH', 'ARMD', 'HR', 'MYA']\n        other_cols = [col for col in all_disease_cols if col not in used_for_mapping]\n        O = 1 if any(row[col] == 1 for col in other_cols) else 0\n\n        harmonized_rows.append({\n            'image_path': image_path,\n            'dataset': 'RFMiD_v1',\n            'split': split,\n            'ID': image_id,\n            'N': N, 'D': D, 'G': G, 'C': C, 'A': A, 'H': H, 'M': M, 'O': O\n        })\n\n    return pd.DataFrame(harmonized_rows)\n\n\ndef harmonize_rfmid_v2(split: str) -> pd.DataFrame:\n    base_path = \"/kaggle/input/rdc-version-2/RFDiM2_0\"\n\n    if split == \"train\":\n        img_dir = f\"{base_path}/Training_set_2/Train_2\"\n        label_file = f\"{base_path}/Training_set_2/RFMiD_2_Training_labels.csv\"\n    elif split == \"val\":\n        img_dir = f\"{base_path}/Validation_set_2/Validation_2\"\n        label_file = f\"{base_path}/Validation_set_2/RFMiD_2_Validation_labels.csv\"\n    else:\n        img_dir = f\"{base_path}/Test_set_2/Test_2\"\n        label_file = f\"{base_path}/Test_set_2/RFMiD_2_Testing_labels.csv\"\n\n    try:\n        df = pd.read_csv(label_file, encoding=\"utf-8\")\n    except UnicodeDecodeError:\n        df = pd.read_csv(label_file, encoding=\"latin1\")\n\n    df.columns = df.columns.str.strip()\n\n    potential_disease_cols = [\n        'AH', 'AION', 'ARMD', 'BRVO', 'CB', 'CF', 'CL', 'CME', 'CNV', 'CRAO',\n        'CRS', 'CRVO', 'CSR', 'CWS', 'CSC', 'DN', 'DR', 'EDN', 'ERM', 'GRT',\n        'HPED', 'HR', 'LS', 'MCA', 'ME', 'MH', 'MHL', 'MS', 'MYA', 'ODC',\n        'ODE', 'ODP', 'ON', 'OPDM', 'PRH', 'RD', 'RHL', 'RTR', 'RP', 'RPEC',\n        'RS', 'RT', 'SOFE', 'ST', 'TD', 'TSLN', 'TV', 'VS', 'HTN', 'IIH'\n    ]\n    all_disease_cols = [c for c in potential_disease_cols if c in df.columns]\n\n    harmonized_rows = []\n    skipped_count, found_count = 0, 0\n\n    for _, row in df.iterrows():\n        image_id = int(row['ID'])\n\n        image_path = None\n        for ext in ['.jpg', '.JPG', '.png', '.PNG', '.jpeg', '.JPEG']:\n            candidate = os.path.join(img_dir, f\"{image_id}{ext}\")\n            if os.path.exists(candidate):\n                image_path = candidate\n                found_count += 1\n                break\n\n        if image_path is None:\n            skipped_count += 1\n            continue\n\n        wnl = row.get('WNL', 0)\n        N = 1 if wnl == 1 else 0\n        D = 1 if row.get('DR', 0) == 1 else 0\n        G = 1 if row.get('ODC', 0) == 1 else 0\n        C = 1 if row.get('MH', 0) == 1 else 0\n        A = 1 if row.get('ARMD', 0) == 1 else 0\n\n        H = 1 if row.get('HTN', 0) == 1 else 0\n        if H == 0 and 'HR' in df.columns:\n            H = 1 if row.get('HR', 0) == 1 else 0\n\n        M = 1 if row.get('MYA', 0) == 1 else 0\n\n        used_for_mapping = ['DR', 'ODC', 'MH', 'ARMD', 'HTN', 'HR', 'MYA', 'WNL']\n        other_cols = [c for c in all_disease_cols if c not in used_for_mapping]\n        O = 1 if any(row.get(c, 0) == 1 for c in other_cols) else 0\n\n        harmonized_rows.append({\n            'image_path': image_path,\n            'dataset': 'RFMiD_v2',\n            'split': split,\n            'ID': image_id,\n            'N': N, 'D': D, 'G': G, 'C': C, 'A': A, 'H': H, 'M': M, 'O': O\n        })\n\n    if skipped_count > 0:\n        print(f\"RFMiD_v2 {split}: Found {found_count}, Skipped {skipped_count}\")\n\n    return pd.DataFrame(harmonized_rows)\n\n\ndef harmonize_all_datasets() -> Dict[str, pd.DataFrame]:\n    results = {}\n    print(\"\\n\" + \"=\" * 60)\n    print(\"HARMONIZING DATASETS\")\n    print(\"=\" * 60)\n\n    for split in ['train', 'val', 'test']:\n        print(f\"\\nProcessing {split} split...\")\n        results[f'ODIR_{split}'] = harmonize_odir(split)\n        results[f'RFMiD_v1_{split}'] = harmonize_rfmid_v1(split)\n        results[f'RFMiD_v2_{split}'] = harmonize_rfmid_v2(split)\n\n    return results\n\n\ndef print_statistics(harmonized_data: Dict[str, pd.DataFrame]):\n    label_cols = ['N', 'D', 'G', 'C', 'A', 'H', 'M', 'O']\n\n    print(\"\\n\" + \"=\" * 60)\n    print(\"DATASET STATISTICS\")\n    print(\"=\" * 60)\n\n    print(\"\\nSample Counts:\")\n    print(\"-\" * 40)\n    for key, df in sorted(harmonized_data.items()):\n        print(f\"{key:20s}: {len(df):5d} images\")\n\n    total_train = sum(len(df) for k, df in harmonized_data.items() if 'train' in k)\n    total_val = sum(len(df) for k, df in harmonized_data.items() if 'val' in k)\n    total_test = sum(len(df) for k, df in harmonized_data.items() if 'test' in k)\n\n    print(\"-\" * 40)\n    print(f\"{'Total Train':20s}: {total_train:5d} images\")\n    print(f\"{'Total Val':20s}: {total_val:5d} images\")\n    print(f\"{'Total Test':20s}: {total_test:5d} images\")\n    print(f\"{'Grand Total':20s}: {total_train + total_val + total_test:5d} images\")\n\n\ndef save_harmonized_data(harmonized_data: Dict[str, pd.DataFrame], output_dir: str = './harmonized_labels'):\n    os.makedirs(output_dir, exist_ok=True)\n\n    print(\"\\n\" + \"=\" * 60)\n    print(\"SAVING HARMONIZED DATA\")\n    print(\"=\" * 60)\n\n    for key, df in harmonized_data.items():\n        output_path = os.path.join(output_dir, f\"{key}.csv\")\n        df.to_csv(output_path, index=False)\n        print(f\"Saved {key:20s}: {len(df):5d} rows -> {output_path}\")\n\n    print(\"\\nCreating combined files...\")\n    for split in ['train', 'val', 'test']:\n        split_dfs = [v for k, v in harmonized_data.items() if k.endswith(f'_{split}')]\n        if split_dfs:\n            combined = pd.concat(split_dfs, ignore_index=True)\n            output_path = os.path.join(output_dir, f\"combined_{split}.csv\")\n            combined.to_csv(output_path, index=False)\n            print(f\"Saved combined_{split:5s}: {len(combined):5d} rows -> {output_path}\")\n\n    print(f\"\\nAll files saved to: {output_dir}\")\n\n\ndef verify_images(harmonized_data: Dict[str, pd.DataFrame]):\n    print(\"\\n\" + \"=\" * 60)\n    print(\"VERIFYING IMAGE PATHS\")\n    print(\"=\" * 60)\n\n    all_good = True\n    for key, df in sorted(harmonized_data.items()):\n        missing = df[~df['image_path'].apply(os.path.exists)]\n        if len(missing) > 0:\n            print(f\"{key:20s}: {len(missing)} missing images\")\n            all_good = False\n        else:\n            print(f\"{key:20s}: All {len(df)} images found\")\n\n    if all_good:\n        print(\"\\nAll image paths verified successfully!\")\n    else:\n        print(\"\\nSome images are missing - check the paths above\")\n\n\n# ============================================================\n# 2) TRAINING CONFIG\n# ============================================================\n\nSEED = 42\nSAVE_DIR = './results_lodo_mixup/fold_test_RFMiD_v1'\nTEST_DOMAIN = \"RFMiD_v1\"\nTRAIN_DOMAINS = \"ODIR + RFMiD_v2\"\nMIXUP_ALPHA = 0.2\n\nTEST_CSV = '/kaggle/working/harmonized_labels/RFMiD_v1_test.csv'\nTRAIN_CSVS = [\n    '/kaggle/working/harmonized_labels/ODIR_train.csv',\n    '/kaggle/working/harmonized_labels/RFMiD_v2_train.csv'\n]\nVAL_CSVS = [\n    '/kaggle/working/harmonized_labels/ODIR_val.csv',\n    '/kaggle/working/harmonized_labels/RFMiD_v2_val.csv'\n]\n\n\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n\n\n# ============================================================\n# Dataset\n# ============================================================\n\nclass RetinalDataset(Dataset):\n    def __init__(self, csv_path, domain_id, transform=None):\n        self.data = pd.read_csv(csv_path)\n        self.domain_id = domain_id\n        self.transform = transform\n        self.label_cols = [\"N\", \"D\", \"G\", \"C\", \"A\", \"H\", \"M\", \"O\"]\n\n        dup = self.data.duplicated(subset=[\"image_path\"], keep=\"first\").sum()\n        if dup > 0:\n            print(f\"[WARN] {dup} duplicates found, keeping first\")\n        self.data = self.data.drop_duplicates(subset=[\"image_path\"]).reset_index(drop=True)\n\n        valid_mask = self.data[\"image_path\"].apply(os.path.exists)\n        missing = int((~valid_mask).sum())\n        if missing > 0:\n            print(f\"[WARN] Dropping {missing} missing images\")\n        self.data = self.data.loc[valid_mask].reset_index(drop=True)\n\n        print(f\"[INFO] Domain {domain_id}: Loaded {len(self.data)} valid images\")\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        row = self.data.iloc[idx]\n        try:\n            image = Image.open(row[\"image_path\"]).convert(\"RGB\")\n        except Exception:\n            image = Image.new(\"RGB\", (224, 224), color=(128, 128, 128))\n\n        labels = torch.tensor(row[self.label_cols].values.astype(\"float32\"))\n\n        if self.transform:\n            image = self.transform(image)\n\n        return image, labels, self.domain_id\n\n\n# ============================================================\n# Pos weights\n# ============================================================\n\ndef calculate_pos_weights(datasets, clip_min=0.5, clip_max=50.0):\n    label_cols = [\"N\", \"D\", \"G\", \"C\", \"A\", \"H\", \"M\", \"O\"]\n    combined = np.vstack([ds.data[label_cols].values for ds in datasets])\n\n    pos = combined.sum(axis=0)\n    neg = len(combined) - pos\n    raw = neg / (pos + 1e-5)\n    clipped = np.clip(raw, clip_min, clip_max)\n\n    print(\"\\n[INFO] Positive class weights (from training domains):\")\n    for i, col in enumerate(label_cols):\n        print(f\"  {col}: {clipped[i]:.2f}\")\n\n    return torch.tensor(clipped, dtype=torch.float32)\n\n\n# ============================================================\n# ConvNeXt preprocessing params (robust across torchvision)\n# ============================================================\n\ndef _to_int_square(x: Any, default: int) -> int:\n    if x is None:\n        return default\n    if isinstance(x, int):\n        return x\n    if isinstance(x, (tuple, list)) and len(x) > 0:\n        return int(x[0])\n    return default\n\n\ndef get_convnext_preprocess_params():\n    weights = ConvNeXt_Tiny_Weights.DEFAULT\n    preset = weights.transforms()  # callable preset\n\n    mean = getattr(preset, \"mean\", None)\n    std = getattr(preset, \"std\", None)\n\n    # fallback\n    if mean is None or std is None:\n        meta = getattr(weights, \"meta\", {}) or {}\n        mean = meta.get(\"mean\", (0.485, 0.456, 0.406))\n        std = meta.get(\"std\", (0.229, 0.224, 0.225))\n\n    crop_size = _to_int_square(getattr(preset, \"crop_size\", None), default=224)\n    resize_size = _to_int_square(getattr(preset, \"resize_size\", None), default=256)\n\n    try:\n        interp = transforms.InterpolationMode.BILINEAR\n    except Exception:\n        interp = 2  # PIL BILINEAR int fallback\n\n    antialias = bool(getattr(preset, \"antialias\", True))\n    return tuple(mean), tuple(std), crop_size, resize_size, interp, antialias\n\n\nMEAN, STD, CROP, RESIZE, INTERP, ANTIALIAS = get_convnext_preprocess_params()\n\n\ndef _safe_resize(size):\n    try:\n        return transforms.Resize(size, interpolation=INTERP, antialias=ANTIALIAS)\n    except TypeError:\n        return transforms.Resize(size, interpolation=INTERP)\n\n\ndef get_transforms(is_train=False):\n    if is_train:\n        return transforms.Compose([\n            _safe_resize((RESIZE, RESIZE)),\n            transforms.RandomCrop(CROP),\n            transforms.RandomRotation(10),\n            transforms.ColorJitter(brightness=0.2, contrast=0.2),\n            transforms.ToTensor(),\n            transforms.Normalize(MEAN, STD),\n        ])\n    else:\n        return transforms.Compose([\n            _safe_resize((RESIZE, RESIZE)),\n            transforms.CenterCrop(CROP),\n            transforms.ToTensor(),\n            transforms.Normalize(MEAN, STD),\n        ])\n\n\n# ============================================================\n# Model: ConvNeXt-Tiny (FIXED FORWARD)\n# ============================================================\n\nclass ConvNeXtTinyMultiLabel(nn.Module):\n    def __init__(self, num_classes=8, dropout=0.3, pretrained=True):\n        super().__init__()\n        weights = ConvNeXt_Tiny_Weights.DEFAULT if pretrained else None\n        self.backbone = models.convnext_tiny(weights=weights)\n\n        # convnext_tiny final feature dim is usually 768\n        self.feature_dim = self.backbone.classifier[-1].in_features  # safe way\n\n        # We'll NOT use the built-in classifier; we use features + avgpool ourselves\n        self.backbone.classifier = nn.Identity()\n\n        self.classifier = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Linear(self.feature_dim, 512),\n            nn.GELU(),\n            nn.BatchNorm1d(512),\n            nn.Dropout(dropout),\n            nn.Linear(512, num_classes),\n        )\n\n    def forward(self, x):\n        # IMPORTANT: use features -> avgpool -> flatten -> classifier\n        x = self.backbone.features(x)      # [B, C, H, W]\n        x = self.backbone.avgpool(x)       # [B, C, 1, 1]\n        x = torch.flatten(x, 1)            # [B, C]\n        return self.classifier(x)          # [B, num_classes]\n\n\n# ============================================================\n# Metrics\n# ============================================================\n\ndef compute_metrics(labels, probs, thresholds=None):\n    n_classes = labels.shape[1]\n\n    aucs = []\n    for i in range(n_classes):\n        if len(np.unique(labels[:, i])) > 1:\n            aucs.append(roc_auc_score(labels[:, i], probs[:, i]))\n        else:\n            aucs.append(np.nan)\n\n    aps = []\n    for i in range(n_classes):\n        if len(np.unique(labels[:, i])) > 1:\n            aps.append(average_precision_score(labels[:, i], probs[:, i]))\n        else:\n            aps.append(np.nan)\n\n    if thresholds is None:\n        thresholds = np.full(n_classes, 0.5)\n\n    preds = (probs >= thresholds).astype(int)\n    f1 = f1_score(labels, preds, average=\"macro\", zero_division=0)\n\n    return {\n        \"mAUC\": float(np.nanmean(aucs)),\n        \"mAP\": float(np.nanmean(aps)),\n        \"per_class_auc\": aucs,\n        \"per_class_ap\": aps,\n        \"macro_f1\": float(f1),\n    }\n\n\ndef find_optimal_thresholds(labels, probs):\n    n_classes = labels.shape[1]\n    thresholds = []\n    search_range = np.linspace(0.05, 0.95, 91)\n\n    for i in range(n_classes):\n        best_f1, best_t = 0.0, 0.5\n        if len(np.unique(labels[:, i])) > 1:\n            for t in search_range:\n                preds = (probs[:, i] >= t).astype(int)\n                f1 = f1_score(labels[:, i], preds, zero_division=0)\n                if f1 > best_f1:\n                    best_f1, best_t = f1, t\n        thresholds.append(best_t)\n\n    return np.array(thresholds)\n\n\n# ============================================================\n# Mixup Training + Validation\n# ============================================================\n\ndef train_epoch_mixup(model, loaders_dict, criterion, optimizer, device, mixup_alpha=0.2):\n    model.train()\n    losses = []\n    mixup_stats = {'total_batches': 0, 'mixup_batches': 0}\n\n    domain_iters = {k: iter(v) for k, v in loaders_dict.items()}\n    domain_ids = list(loaders_dict.keys())\n\n    max_batches = max(len(loader) for loader in loaders_dict.values())\n    pbar = tqdm(range(max_batches), desc=\"Train (Mixup)\", leave=False)\n\n    for _ in pbar:\n        do_mixup = (len(domain_ids) >= 2) and (np.random.rand() > 0.5)\n\n        if do_mixup:\n            d1, d2 = np.random.choice(domain_ids, size=2, replace=False)\n\n            try:\n                imgs1, labels1, _ = next(domain_iters[d1])\n            except StopIteration:\n                domain_iters[d1] = iter(loaders_dict[d1])\n                imgs1, labels1, _ = next(domain_iters[d1])\n\n            try:\n                imgs2, labels2, _ = next(domain_iters[d2])\n            except StopIteration:\n                domain_iters[d2] = iter(loaders_dict[d2])\n                imgs2, labels2, _ = next(domain_iters[d2])\n\n            min_size = min(imgs1.size(0), imgs2.size(0))\n            imgs1, labels1 = imgs1[:min_size], labels1[:min_size]\n            imgs2, labels2 = imgs2[:min_size], labels2[:min_size]\n\n            lam = np.random.beta(mixup_alpha, mixup_alpha)\n\n            mixed_imgs = lam * imgs1 + (1 - lam) * imgs2\n            mixed_labels = lam * labels1 + (1 - lam) * labels2\n\n            mixed_imgs = mixed_imgs.to(device)\n            mixed_labels = mixed_labels.to(device)\n\n            optimizer.zero_grad()\n            logits = model(mixed_imgs)\n            loss = criterion(logits, mixed_labels)\n\n            mixup_stats['mixup_batches'] += 1\n\n        else:\n            d = np.random.choice(domain_ids)\n            try:\n                imgs, labels, _ = next(domain_iters[d])\n            except StopIteration:\n                domain_iters[d] = iter(loaders_dict[d])\n                imgs, labels, _ = next(domain_iters[d])\n\n            imgs = imgs.to(device)\n            labels = labels.to(device)\n\n            optimizer.zero_grad()\n            logits = model(imgs)\n            loss = criterion(logits, labels)\n\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n\n        losses.append(loss.item())\n        mixup_stats['total_batches'] += 1\n        pbar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n\n    return {\n        \"loss\": float(np.mean(losses)),\n        \"mixup_ratio\": mixup_stats['mixup_batches'] / mixup_stats['total_batches']\n    }\n\n\n@torch.no_grad()\ndef validate(model, loader, criterion, device, thresholds=None):\n    model.eval()\n    losses, all_probs, all_labels = [], [], []\n\n    for batch in tqdm(loader, desc=\"Val\", leave=False):\n        if len(batch) == 3:\n            images, labels, _ = batch\n        else:\n            images, labels = batch\n\n        images = images.to(device)\n        labels = labels.to(device)\n\n        logits = model(images)\n        loss = criterion(logits, labels)\n\n        losses.append(loss.item())\n        all_probs.append(torch.sigmoid(logits).cpu().numpy())\n        all_labels.append(labels.cpu().numpy())\n\n    probs = np.vstack(all_probs)\n    labels = np.vstack(all_labels)\n    metrics = compute_metrics(labels, probs, thresholds)\n    metrics[\"loss\"] = float(np.mean(losses))\n    return metrics, probs, labels\n\n\n# ============================================================\n# MAIN\n# ============================================================\n\nif __name__ == \"__main__\":\n    # If you already created harmonized CSVs, comment these out.\n    harmonized_data = harmonize_all_datasets()\n    print_statistics(harmonized_data)\n    verify_images(harmonized_data)\n    save_harmonized_data(harmonized_data, output_dir=\"./harmonized_labels\")\n\n    set_seed(SEED)\n    os.makedirs(SAVE_DIR, exist_ok=True)\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n    print(\"=\" * 80)\n    print(f\"LODO FOLD with MIXUP DG: Test on {TEST_DOMAIN}\")\n    print(f\"Training on: {TRAIN_DOMAINS}\")\n    print(\"=\" * 80)\n    print(f\"Device: {device}\")\n    print(f\"Seed: {SEED}\")\n    print(f\"Mixup Alpha: {MIXUP_ALPHA}\")\n    print(\"=\" * 80)\n\n    print(\"\\n[INFO] Loading datasets...\")\n    train_datasets = [RetinalDataset(csv, domain_id=i, transform=get_transforms(True))\n                      for i, csv in enumerate(TRAIN_CSVS)]\n    val_datasets = [RetinalDataset(csv, domain_id=i, transform=get_transforms(False))\n                    for i, csv in enumerate(VAL_CSVS)]\n    test_dataset = RetinalDataset(TEST_CSV, domain_id=999, transform=get_transforms(False))\n\n    print(f\"\\n[INFO] Total train: {sum(len(ds) for ds in train_datasets)}\")\n    print(f\"[INFO] Total val:   {sum(len(ds) for ds in val_datasets)}\")\n    print(f\"[INFO] Test:        {len(test_dataset)}\")\n\n    pos_weights = calculate_pos_weights(train_datasets).to(device)\n\n    g = torch.Generator().manual_seed(SEED)\n    train_loaders = {\n        i: DataLoader(ds, batch_size=32, shuffle=True, num_workers=4,\n                      pin_memory=True, generator=g)\n        for i, ds in enumerate(train_datasets)\n    }\n\n    combined_val = ConcatDataset(val_datasets)\n    val_loader = DataLoader(combined_val, batch_size=32, shuffle=False,\n                            num_workers=4, pin_memory=True)\n    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False,\n                             num_workers=4, pin_memory=True)\n\n    print(\"\\n[INFO] Initializing ConvNeXt-Tiny model...\")\n    model = ConvNeXtTinyMultiLabel(num_classes=8, dropout=0.3, pretrained=True).to(device)\n\n    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weights)\n    optimizer = optim.AdamW(model.parameters(), lr=2e-4, weight_decay=1e-4)\n    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3)\n\n    history = {'train_loss': [], 'val_loss': [], 'val_auc': []}\n    best_val_auc, patience_counter = 0.0, 0\n\n    print(\"\\n[INFO] Training started...\")\n    print(\"-\" * 80)\n\n    for epoch in range(50):\n        train_metrics = train_epoch_mixup(model, train_loaders, criterion, optimizer, device,\n                                          mixup_alpha=MIXUP_ALPHA)\n        val_metrics, _, _ = validate(model, val_loader, criterion, device)\n        scheduler.step(val_metrics['mAUC'])\n\n        history['train_loss'].append(train_metrics['loss'])\n        history['val_loss'].append(val_metrics['loss'])\n        history['val_auc'].append(val_metrics['mAUC'])\n\n        print(f\"Epoch {epoch+1:02d} | Train Loss: {train_metrics['loss']:.4f} \"\n              f\"| Val Loss: {val_metrics['loss']:.4f} | Val mAUC: {val_metrics['mAUC']:.4f} \"\n              f\"| Mixup: {train_metrics['mixup_ratio']:.1%}\")\n\n        if val_metrics['mAUC'] > best_val_auc:\n            best_val_auc = val_metrics['mAUC']\n            torch.save(model.state_dict(), f'{SAVE_DIR}/best_model.pth')\n            print(f\"  [OK] Saved best model (val mAUC: {best_val_auc:.4f})\")\n            patience_counter = 0\n        else:\n            patience_counter += 1\n\n        if patience_counter >= 10:\n            print(f\"\\n[INFO] Early stopping at epoch {epoch+1}\")\n            break\n\n    print(f\"\\n[INFO] Loading best model...\")\n    model.load_state_dict(torch.load(f'{SAVE_DIR}/best_model.pth', map_location=device))\n    print(f\"[INFO] Best validation mAUC: {best_val_auc:.4f}\")\n\n    print(\"\\n[INFO] Finding optimal thresholds on validation...\")\n    _, val_probs, val_labels = validate(model, val_loader, criterion, device)\n    thresholds = find_optimal_thresholds(val_labels, val_probs)\n\n    class_names = [\"N\", \"D\", \"G\", \"C\", \"A\", \"H\", \"M\", \"O\"]\n    print(\"[INFO] Optimal thresholds:\")\n    for c, t in zip(class_names, thresholds):\n        print(f\"  {c}: {t:.3f}\")\n\n    print(f\"\\n[INFO] Testing on {TEST_DOMAIN}...\")\n    test_metrics, _, _ = validate(model, test_loader, criterion, device, thresholds)\n\n    print(\"\\n\" + \"=\" * 80)\n    print(f\"TEST RESULTS - {TEST_DOMAIN} (ConvNeXt-Tiny + Mixup)\")\n    print(\"=\" * 80)\n    print(f\"mAUC:      {test_metrics['mAUC']:.4f}\")\n    print(f\"mAP:       {test_metrics['mAP']:.4f}\")\n    print(f\"Macro F1:  {test_metrics['macro_f1']:.4f}\")\n    print(\"=\" * 80)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-01-17T22:40:28.380694Z","iopub.execute_input":"2026-01-17T22:40:28.381254Z","iopub.status.idle":"2026-01-17T23:33:27.546222Z","shell.execute_reply.started":"2026-01-17T22:40:28.381209Z","shell.execute_reply":"2026-01-17T23:33:27.545392Z"}},"outputs":[{"name":"stdout","text":"\n============================================================\nHARMONIZING DATASETS\n============================================================\n\nProcessing train split...\nRFMiD_v2 train: Found 507, Skipped 2\n\nProcessing val split...\n\nProcessing test split...\nRFMiD_v2 test: Found 170, Skipped 4\n\n============================================================\nDATASET STATISTICS\n============================================================\n\nSample Counts:\n----------------------------------------\nODIR_test           :  2000 images\nODIR_train          :  7000 images\nODIR_val            :  1000 images\nRFMiD_v1_test       :   640 images\nRFMiD_v1_train      :  1920 images\nRFMiD_v1_val        :   640 images\nRFMiD_v2_test       :   170 images\nRFMiD_v2_train      :   507 images\nRFMiD_v2_val        :   177 images\n----------------------------------------\nTotal Train         :  9427 images\nTotal Val           :  1817 images\nTotal Test          :  2810 images\nGrand Total         : 14054 images\n\n============================================================\nVERIFYING IMAGE PATHS\n============================================================\nODIR_test           : All 2000 images found\nODIR_train          : All 7000 images found\nODIR_val            : All 1000 images found\nRFMiD_v1_test       : All 640 images found\nRFMiD_v1_train      : All 1920 images found\nRFMiD_v1_val        : All 640 images found\nRFMiD_v2_test       : All 170 images found\nRFMiD_v2_train      : All 507 images found\nRFMiD_v2_val        : All 177 images found\n\nAll image paths verified successfully!\n\n============================================================\nSAVING HARMONIZED DATA\n============================================================\nSaved ODIR_train          :  7000 rows -> ./harmonized_labels/ODIR_train.csv\nSaved RFMiD_v1_train      :  1920 rows -> ./harmonized_labels/RFMiD_v1_train.csv\nSaved RFMiD_v2_train      :   507 rows -> ./harmonized_labels/RFMiD_v2_train.csv\nSaved ODIR_val            :  1000 rows -> ./harmonized_labels/ODIR_val.csv\nSaved RFMiD_v1_val        :   640 rows -> ./harmonized_labels/RFMiD_v1_val.csv\nSaved RFMiD_v2_val        :   177 rows -> ./harmonized_labels/RFMiD_v2_val.csv\nSaved ODIR_test           :  2000 rows -> ./harmonized_labels/ODIR_test.csv\nSaved RFMiD_v1_test       :   640 rows -> ./harmonized_labels/RFMiD_v1_test.csv\nSaved RFMiD_v2_test       :   170 rows -> ./harmonized_labels/RFMiD_v2_test.csv\n\nCreating combined files...\nSaved combined_train:  9427 rows -> ./harmonized_labels/combined_train.csv\nSaved combined_val  :  1817 rows -> ./harmonized_labels/combined_val.csv\nSaved combined_test :  2810 rows -> ./harmonized_labels/combined_test.csv\n\nAll files saved to: ./harmonized_labels\n================================================================================\nLODO FOLD with MIXUP DG: Test on RFMiD_v1\nTraining on: ODIR + RFMiD_v2\n================================================================================\nDevice: cuda\nSeed: 42\nMixup Alpha: 0.2\n================================================================================\n\n[INFO] Loading datasets...\n[INFO] Domain 0: Loaded 7000 valid images\n[INFO] Domain 1: Loaded 507 valid images\n[INFO] Domain 0: Loaded 1000 valid images\n[INFO] Domain 1: Loaded 177 valid images\n[INFO] Domain 999: Loaded 640 valid images\n\n[INFO] Total train: 7507\n[INFO] Total val:   1177\n[INFO] Test:        640\n\n[INFO] Positive class weights (from training domains):\n  N: 2.08\n  D: 2.27\n  G: 15.65\n  C: 15.68\n  A: 21.54\n  H: 26.80\n  M: 19.02\n  O: 2.39\n\n[INFO] Initializing ConvNeXt-Tiny model...\nDownloading: \"https://download.pytorch.org/models/convnext_tiny-983f1562.pth\" to /root/.cache/torch/hub/checkpoints/convnext_tiny-983f1562.pth\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 109M/109M [00:00<00:00, 205MB/s] \n","output_type":"stream"},{"name":"stdout","text":"\n[INFO] Training started...\n--------------------------------------------------------------------------------\n","output_type":"stream"},{"name":"stderr","text":"                                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 01 | Train Loss: 0.9446 | Val Loss: 0.9234 | Val mAUC: 0.8163 | Mixup: 53.9%\n  [OK] Saved best model (val mAUC: 0.8163)\n","output_type":"stream"},{"name":"stderr","text":"                                                                             ","output_type":"stream"},{"name":"stdout","text":"Epoch 02 | Train Loss: 0.7124 | Val Loss: 0.9093 | Val mAUC: 0.8096 | Mixup: 44.3%\n","output_type":"stream"},{"name":"stderr","text":"                                                                             ","output_type":"stream"},{"name":"stdout","text":"Epoch 03 | Train Loss: 0.6028 | Val Loss: 0.8906 | Val mAUC: 0.8099 | Mixup: 51.6%\n","output_type":"stream"},{"name":"stderr","text":"                                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 04 | Train Loss: 0.5717 | Val Loss: 0.8895 | Val mAUC: 0.8349 | Mixup: 48.4%\n  [OK] Saved best model (val mAUC: 0.8349)\n","output_type":"stream"},{"name":"stderr","text":"                                                                             ","output_type":"stream"},{"name":"stdout","text":"Epoch 05 | Train Loss: 0.5602 | Val Loss: 0.9278 | Val mAUC: 0.8150 | Mixup: 50.2%\n","output_type":"stream"},{"name":"stderr","text":"                                                                             ","output_type":"stream"},{"name":"stdout","text":"Epoch 06 | Train Loss: 0.5532 | Val Loss: 0.9737 | Val mAUC: 0.8334 | Mixup: 46.6%\n","output_type":"stream"},{"name":"stderr","text":"                                                                             ","output_type":"stream"},{"name":"stdout","text":"Epoch 07 | Train Loss: 0.5029 | Val Loss: 1.1131 | Val mAUC: 0.8181 | Mixup: 53.4%\n","output_type":"stream"},{"name":"stderr","text":"                                                                             ","output_type":"stream"},{"name":"stdout","text":"Epoch 08 | Train Loss: 0.4517 | Val Loss: 1.0811 | Val mAUC: 0.8156 | Mixup: 46.6%\n","output_type":"stream"},{"name":"stderr","text":"                                                                             ","output_type":"stream"},{"name":"stdout","text":"Epoch 09 | Train Loss: 0.4816 | Val Loss: 1.0919 | Val mAUC: 0.8327 | Mixup: 52.1%\n","output_type":"stream"},{"name":"stderr","text":"                                                                             ","output_type":"stream"},{"name":"stdout","text":"Epoch 10 | Train Loss: 0.3712 | Val Loss: 1.1169 | Val mAUC: 0.8328 | Mixup: 52.5%\n","output_type":"stream"},{"name":"stderr","text":"                                                                             ","output_type":"stream"},{"name":"stdout","text":"Epoch 11 | Train Loss: 0.3890 | Val Loss: 1.1644 | Val mAUC: 0.8325 | Mixup: 48.4%\n","output_type":"stream"},{"name":"stderr","text":"                                                                             ","output_type":"stream"},{"name":"stdout","text":"Epoch 12 | Train Loss: 0.3965 | Val Loss: 1.2922 | Val mAUC: 0.8189 | Mixup: 51.1%\n","output_type":"stream"},{"name":"stderr","text":"                                                                             ","output_type":"stream"},{"name":"stdout","text":"Epoch 13 | Train Loss: 0.3155 | Val Loss: 1.2657 | Val mAUC: 0.8218 | Mixup: 47.5%\n","output_type":"stream"},{"name":"stderr","text":"                                                                             \r","output_type":"stream"},{"name":"stdout","text":"Epoch 14 | Train Loss: 0.3315 | Val Loss: 1.2757 | Val mAUC: 0.8327 | Mixup: 52.1%\n\n[INFO] Early stopping at epoch 14\n\n[INFO] Loading best model...\n[INFO] Best validation mAUC: 0.8349\n\n[INFO] Finding optimal thresholds on validation...\n","output_type":"stream"},{"name":"stderr","text":"                                                    \r","output_type":"stream"},{"name":"stdout","text":"[INFO] Optimal thresholds:\n  N: 0.400\n  D: 0.200\n  G: 0.770\n  C: 0.950\n  A: 0.870\n  H: 0.420\n  M: 0.830\n  O: 0.450\n\n[INFO] Testing on RFMiD_v1...\n","output_type":"stream"},{"name":"stderr","text":"                                                    ","output_type":"stream"},{"name":"stdout","text":"\n================================================================================\nTEST RESULTS - RFMiD_v1 (ConvNeXt-Tiny + Mixup)\n================================================================================\nmAUC:      0.8414\nmAP:       0.4879\nMacro F1:  0.3586\n================================================================================\n","output_type":"stream"},{"name":"stderr","text":"\r","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}